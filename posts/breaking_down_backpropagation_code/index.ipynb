{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cd307e7c-6c4e-4492-8b20-4130d218ee4e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Breaking down backpropagation implementation\"\n",
    "author: \"Hamza ES-SAMAALI\"\n",
    "categories: [fastai, ml, backpropagation]\n",
    "date: \"2023-08-17\"\n",
    "code-block-bg: true\n",
    "code-block-border-left: \"#31BAE9\"\n",
    "image: \"backprop.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0fa956-8f76-469a-a499-88f643ffa2d1",
   "metadata": {},
   "source": [
    "I have always had this bad habit while learning where I would just take the informations in passively and not try to apply any of it, even while studying math I only read the theorems and never do exercises. \n",
    "\n",
    "This is a very bad habit that I am trying to get rid of lately while doing the [fastai course](https://course.fast.ai/). I try my best not to let anything slide without explicitly understanding it.  \n",
    "\n",
    "Or that's what I thought. Because recently while browsing [the fastai forum](https://forums.fast.ai) I stumbled upon a question the code of backpropagation. A concept that I thought I understood very well. The question was and I quote:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8728acd-bf4c-41b4-b4f6-30e9b6504da7",
   "metadata": {},
   "source": [
    "## Question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e65191-2c71-4d39-8db2-632dd19012e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp,self.targ = inp,targ\n",
    "        self.out = mse(inp, targ)\n",
    "        return self.out \n",
    "\n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]\n",
    "\n",
    "\n",
    "class Lin():\n",
    "\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = lin(inp, self.w, self.b)\n",
    "        return self.out\n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = self.inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85abf208-b0b2-4a2e-99b5-ba49ddcf6260",
   "metadata": {},
   "source": [
    "Shouldnâ€™t it be `self.out.g` instead of `self.inp.g` in the backward definition of `Mse` class. I don't know how `Lin backward()` automatically gets `self.out.g` value. Can some one explain?\n",
    "\n",
    "**End of Quote**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553664e2-6f2a-41af-a406-55505b5ac90b",
   "metadata": {},
   "source": [
    "When I read this question I realized that I didn't have the answer so I revisited the video for the [lesson 13](https://course.fast.ai/Lessons/lesson13.html) and came up with a \"detailed\" response. Here it is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abb4111-51e7-46a1-b690-466eb2e7ce8f",
   "metadata": {},
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86930370-cb68-4b6a-b873-98606b9b8f89",
   "metadata": {},
   "source": [
    "To understand this let us first set all the code we need, then we will take it execute it step by step.\n",
    "Our building blocks are:\n",
    "- the lin function: `def lin(x, w, b): return x@w + b`\n",
    "- the mse function: `def mse(output,target): return ((output.squeeze()-target)**2).mean()`\n",
    "and the classes: `Mse()`, `ReLU()`, `Lin()` and `Model()`\n",
    "Now to create our model and compute backpropagation we run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3764e8d-1895-4e23-9dec-f1e17ce3b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4cef7f-40e4-47be-8a07-aed96cb88461",
   "metadata": {},
   "source": [
    "what happens now?: \n",
    "We are calling the `Model` constructor so if we look inside the object `model` we will find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e91f1-0c80-4550-9736-e66ea3115de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers = [Lin(w1,b1),Relu(),Lin(w2,b2)]\n",
    "model.loss = Mse() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb01db-2d5a-453f-8f48-25c5c21edf7a",
   "metadata": {},
   "source": [
    "Let's name our layers L1, R, and L2 to make the explanation easier to follow.\n",
    "so `L1.w = w1`, `L1.b = b1`, `L2.w = w2` and `L2.b = b2`.  \n",
    "\n",
    "Now let's execute the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260d9e8-a80e-4254-8562-1d88193a387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab24e2-dd94-40e0-997a-a979bf17bd43",
   "metadata": {},
   "source": [
    "here we are using the `model` object as if it was a function, this will trigger the `__call__` method, here is the code for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5134ed7c-cb52-4118-a6a6-3e84a16e9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d86c429-b5dc-4921-b4b5-964ec74878fc",
   "metadata": {},
   "source": [
    "let's execute it:\n",
    "in our case `x = x_train` and `targ = y_train`\n",
    "now let's go through that for loop: `for l in self.layers: x = l(x)`\n",
    "the contents of `model.layers` is `[L1,R,L2]`\n",
    "so the first instruction will be: `x = L1(x)`\n",
    "similarly here again we are using `L1` as function so let's go see what's in its `__call__` method and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd862e-72c0-47e3-9c9a-8e9355364c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lin Call method\n",
    "def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = lin(inp, self.w, self.b)\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43850bfd-20de-4345-bb06-b1860a1fbee7",
   "metadata": {},
   "source": [
    "so we are assigning `inp` to `self.inp`, in this case `L1.inp = x_train` and `L1.out = lin(inp, w1,b1) = x_train @ w1 + b1`.  \n",
    "The call method returns `self.out` so the new value of `x` will be `x = L1.out`.\n",
    "\n",
    "The first iteration of the loop is done, next element is the layer R, so `x = R(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb47d21-b5d0-4ac8-bd34-6f65014148a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU call method\n",
    "def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f40fcc-0fca-446a-99b1-dd7488e51762",
   "metadata": {},
   "source": [
    "so now we have `R.inp = L1.out` `R.out = relu(L1.inp) # basically equal to L1.inp when it's > 0, 0 otherwise`.  \n",
    "Now the new value of `x` is `x = relu(L1.inp)`\n",
    "The second iteration is done, next element is the layer L2, so `x = L2(x)`\n",
    "now we have `L2.inp = relu(L1.inp)` and `L2.out = relu(L1.inp) @ w2 + b2`.  \n",
    "The new value of `x` is `x = L2.out = relu(L1.inp) @ w2 + b2`.  \n",
    "\n",
    "The for loop has ended. Let's go to the next line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10295e60-4e08-43a4-9768-5645ce763354",
   "metadata": {},
   "outputs": [],
   "source": [
    "return self.loss(x, targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dfc984-33d3-43e6-a4da-cacc07ae67f2",
   "metadata": {},
   "source": [
    "We saw earlier that `model.loss = Mse()` so we are using the `__call__` method of the `Mse` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d43723-7beb-46d3-b268-7ef85a645d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call method of the Mse class\n",
    "def __call__(self, inp, targ):\n",
    "        self.inp, self.targ = inp, targ\n",
    "        self.out = mse(inp, targ)\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f354300-45e1-4106-a635-f3bdb9be8771",
   "metadata": {},
   "source": [
    "now we have `mse.inp = x, mse.targ = targ` and `mse.out = mse(x, targ) = ((x.squeeze()-targ)**2).mean()`.  \n",
    "The method return mse.out so `loss = mse.out`.  \n",
    "\n",
    "Finally we get to the part which confused us both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba9946-644a-482a-a2f4-99e1750b1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc04628-c637-4bbf-a906-62f85eccda38",
   "metadata": {},
   "source": [
    "it calls the `backward` method of the `Model` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24776dcf-e578-459f-9618-5a04b0e680c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward method of the Model class\n",
    "def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08497a81-b58d-4985-91c5-b573762a82fc",
   "metadata": {},
   "source": [
    "In the first line we have `model.loss.backward()` which is none other than the `backward` method of the `Mse` class. because remember that `loss` is an instance of the `Mse` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af93ec6-7e2a-4fa8-bd08-f900e9850649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward method of Mse\n",
    "def backward(self):\n",
    "        self.inp.g = 2 * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.inp.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98e851-27bf-4ec2-9d3c-521857e7eee4",
   "metadata": {},
   "source": [
    "So here we compute `mse.inp.g` and we saw earlier that `mse.ing = x` so we are in fact computing `x.g` and it's equal to `x.g = 2 * (x.squeeze() - targ).unsqueeze(-1) / x.shape[0]`  \n",
    "\n",
    "`x` as you know is the output of our MLP (multi level perceptron), and the gradient of the loss with respect to the output is stored in the output tensor i.e `x.g`. So that's why it should be indeed `inp.g` and not `out.g` in the `backward` method of the `Mse` class.  \n",
    "\n",
    "Now in order to find out how `backward` of `Lin` get the `out.g` value let's continue executing our code. We have have executed the first line now let's run the for loop:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aed11e-eedc-4165-aef5-eef9b4813e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c545b9-faba-4ea6-8c96-c3cbd22f99b6",
   "metadata": {},
   "source": [
    "the first value of `l` is `L2` (because we are going through the reversed list of layers)\n",
    "so let's run `L2.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc254b0-ac23-4de8-a786-34271e427e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lin backward\n",
    "def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = self.inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326bf91f-df6d-4519-b689-ac429712cb20",
   "metadata": {},
   "source": [
    "We already know that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef830f9e-8f7d-4568-95eb-407a3670251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2.inp = relu(L1.inp)\n",
    "L2.out = relu(L1.inp) @ w2 + b2 = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eaaeda-7c29-458d-b8a6-3514a365c867",
   "metadata": {},
   "source": [
    "so when we call `L2.backward()` this method will perform the following updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ff523-fc5b-42b6-9b3c-348033538631",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2.inp.g =  L2.out.g @ L2.w.t() # which is equivalent to L2.inp.g = x.g @ w2.t() \n",
    "w2.g = L2.inp.t() @ L2.out.g\n",
    "b2.g = L2.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09b23d-9b23-4eb7-af39-16c128980016",
   "metadata": {},
   "source": [
    "As you can see `Lin` knows automatically what `out.g` is, because when we ran `model.loss.backward()` we calculated it. \n",
    "So now we have computed `L2.inp.g` (which is `R.out.g`) ,`w2.g` and `b2.g`.  \n",
    "The first iteration of the loop has ended, next `l=R` and we will run `R.backward`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5debc3-f944-44a3-9596-d84533af1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self): self.inp.g = (self.inp>0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e606cc7-15d0-4c45-bb56-afd6108ab10b",
   "metadata": {},
   "source": [
    "We know that `R.inp = L1.out` and `R.out = relu(L1.inp)`\n",
    "The following updates will occur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ad22a0-f511-4668-9d7c-1ab83a8a9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "R.inp.g = (R.inp > 0).float() * R.out.g "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e7cf7-4c4a-4451-9dcb-6b61217eb2a9",
   "metadata": {},
   "source": [
    "Now we have computed `R.inp.g` (which is `L1.out.g`).  \n",
    "This iteration is done, next is `l = L1` so we will call `L1.backward()`.  \n",
    "We know that `L1.inp = x_train` and that `L1.out  = R.inp`\n",
    "So calling `backward` of `L1` will give us the following updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd140c93-9eae-4dd3-89ee-ab62845f1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1.inp.g =  L1.out.g @ w1.t() # which is equivalent to L1.inp.g = R.inp.g @ w1.t() \n",
    "w1.g = L1.inp.t() @ L1.out.g\n",
    "b1.g = L1.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf2bd07-413e-4ccd-8a19-7a4c79afbc78",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c904f0-c89a-4713-b711-33201b80e5ae",
   "metadata": {},
   "source": [
    "The main takeaway is that backpropagation strats at the end and computes the gradient of the loss and stores it in the output tensor of the neural network (which is the input tensor of the loss function, and that's what's confusing).  \n",
    "\n",
    "I really hope that you may find this blog post useful. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
