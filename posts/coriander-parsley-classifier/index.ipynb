{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e8f229d7-0d14-4f04-bc63-022eb3fb7462",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A coriander قزبر vs معدنوس Parsley Classifier\"\n",
    "listing: default\n",
    "author: \"Hamza\"\n",
    "date: \"2023-02-08\"\n",
    "categories: [fastai, ml, classification]\n",
    "image: \"cvp.jpg\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73f9a9-edc2-4a0b-90e2-4eecf7de6b1e",
   "metadata": {},
   "source": [
    "Coriander قزبر and parsley معدنوس are two staple ingredients of Moroccan cuisine, our moms have the superpower of easily distinguishing between them, but us mortals cannot.  \n",
    "\n",
    "In lesson 2 of Deep Learning for Coders 2022 part 1, Jeremy presented a simple image classification model for grizzly, black and teddy bears. I will be using a similar model to help us on this strenuous and confusing task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394dd5c-12e1-44a5-bf06-247f93dbc8d4",
   "metadata": {},
   "source": [
    "# Step 1: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bb2d62-cf5e-4065-8e7b-e300f5869039",
   "metadata": {},
   "source": [
    "First we need to gather the data and train a model to classify our targets. We begin first by important all the relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d9c6c-5f76-47b1-918e-2352a343a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook\n",
    "from fastbook import *\n",
    "from fastai.vision.widgets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3033c2-d291-49a1-9d98-3bafe95f7f26",
   "metadata": {},
   "source": [
    "We then search on DuckDuckGo for images of both coriander and parsley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed16b1a-8bc1-4af3-818b-2bd47ba27abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plant_types = 'coriander','parsley'\n",
    "path = Path('plants')\n",
    "if not path.exists():\n",
    "    path.mkdir()\n",
    "    for o in plant_types:\n",
    "        dest = (path/o)\n",
    "        dest.mkdir(exist_ok=True)\n",
    "        results = search_images_ddg(f'{o}')\n",
    "        download_images(dest, urls=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7148b-4f32-4622-830e-6f3d62f7c939",
   "metadata": {},
   "source": [
    "`plant_types` contains our two category names  \n",
    "`path` is the path where our images will be downloaded  \n",
    "Then we loop over our categories and create respective folders for each of them  \n",
    "We then perform a DuckDuckGo image search with the function `search_images_ddg` and we store all the urls of the results in the variable `results`, we then download all the images in the category folder.  \n",
    "\n",
    "It's standard procedure to check for images that failed to download using the following bit of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf43ec-98d2-48f6-8bfe-a89c28c93d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = get_image_files(path)\n",
    "failed = verify_images(fns)\n",
    "failed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e624d7-123d-4ec8-a0f8-63e527d429a1",
   "metadata": {},
   "source": [
    "we then use `unlink` to remove said failked images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d46cb-4f02-4056-bccf-465a3ec77756",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed.map(Path.unlink)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22add49e-9d0e-4d3c-bbfa-e2c7e296fe09",
   "metadata": {},
   "source": [
    "One of the amazing features of fastai are `Datablocks` and `DataLoaders`.  \n",
    "\n",
    "A Data block is nothing more than a pipeline for data assembly. When you initially create a `DataBlock`, you won’t need to specify any data. What you will need to specify, however, is a set of rules for how to treat your data when it does flow in. It doesn’t care about what you’ll do with it, it just cares about how you want it gathered, classified and split.\n",
    "\n",
    "In order to create a Data block you need to specify  \n",
    "\n",
    "    - what types of data to expect for your input (aka features) and target variables (aka labels)  \n",
    "    - how to get the data  \n",
    "    - how to differentiate features from the target variables,  \n",
    "    - how to split the data for training (train & validation set)  \n",
    "\n",
    "Let’s see how to do that. In our case our DataBlock looks like this:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c105e3cf-2e96-4a8a-900e-8a8211407ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "plants = DataBlock(\n",
    "    blocks = (ImageBlock, CategoryBlock),\n",
    "    get_items = get_image_files,\n",
    "    splitter = RandomSplitter(valid_pct = 0.2, seed = 42),\n",
    "    get_y = parent_label,\n",
    "    item_tfms = Resize(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6172986d-978e-4ef7-a3e7-e1e9cb913cee",
   "metadata": {},
   "source": [
    "The four main steps mentioned above are exactly the first four (required) arguments of a DataBlock:  \n",
    "\n",
    "- `blocks`: is where you specify the types of data your model will work with. Usually you will specify at least two blocks: one that represents your independent (input) variable, and one that represents your dependent (target) variable. You can also specify multiple input/output variables.  \n",
    "- `get_items`: a function that will actually go and pick up the data when necessary (more on this later)  \n",
    "- `splitter`: how the data will be split between training and validation set. The seed is optional and only added for reproducibility.   \n",
    "- `get_y`: how to extract the target (dependent) variable from the data. In our case, this will be by looking at the parent folder, fast.ai provides a built in function called parent_label.  \n",
    "- `item_tfms`: is an optional argument that we can include to specify any additional processing that needs to be carried out when we flow our data through. In this case, we will resize all images to 128x128. We can specify other transforms, such as item_tfms=Resize(128, ResizeMethod.Squish)) which will resize and squish our images to fit, or item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros') to resize and pad any leftover space with black. This method is incredibly powerful as it also supports data augmentation. This is beyond the scope of this blog post, but just know that item_tfms allows you to pre-process your data before it hits your model.  \n",
    "\n",
    "Now that we’ve defined a `DataBlock`, and we’ve specified exactly how our data needs to be structured, categorized and processed, we can start actually feeding in the data for our model to train on. We load this data in with a Data loader. This is where `DataLoaders` come in. A `DataLoaders` is an iterator class that our `DataBlock` will call to load data according to the rules that we’ve specified in specific chunks (called batch size).  \n",
    "\n",
    "A `DataLoader` in fast.ai is a superset of the PyTorch `DataLoader`, with more helpful callbacks and flexibility. Whereas the Data block knows how to structure the data, the Loader knows how to work with it in the context of training a machine learning model i.e. how much to feed to the model at once (batch size), how many processes to spawn to load the data, how much memory to allocate and many more.  \n",
    "\n",
    "A `DataLoaders` (note the plural), is a thin class that automatically generates multiple `DataLoader` (singular) objects based on the rules specified in our `DataBlock`.  \n",
    "\n",
    "Now let's create our own:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd6043-5108-4eba-a07d-49b0b150e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('plants')\n",
    "dls = plants.dataloaders(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a808e4-eea5-4252-b2a9-b28b64b22e8a",
   "metadata": {},
   "source": [
    "our `dataloaders` are now created, let's visualize some of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d6d611-b0e8-4635-ab58-a09ecbcbecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.show_batch(max_n=4, nrows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f0fdc-c1db-48fd-b06e-4a70ffe7d4fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](dataloaders.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b4b10-fe98-4023-99ae-4ac34d10e3d5",
   "metadata": {},
   "source": [
    "Now let's train our model by using the `vision_learner`, but first what is that?  \n",
    "A `learner` is a fastai component that groups together a model, some `Dataloaders` and a `loss_function` to handle training.\n",
    "A `vision_learner` is a `learner` that handles computer vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367bd3fd-4b50-4bd7-9fd1-c63b920a6ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, resnet18, metrics=error_rate)\n",
    "learn.fine_tune(4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931a298-eba3-49c2-8126-68713caf259d",
   "metadata": {},
   "source": [
    "this `vision_learner` will use the `DataLoaders` we created, and will also use a pretrained model `resnet18` and `error_rate` (1 - accuracy) as `metric`.  \n",
    "Then we use `fine_tune` to fine tune our pretrained model to our coriander and parsley data for 4 `epochs`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb044ee6-3f67-46d4-92d6-7c92ae3c3ae7",
   "metadata": {},
   "source": [
    "If you've already done a classification task, you may be shrugging right now and saying \"What is he doing? training a model without any data cleaning?\".  \n",
    "That's what I said myself when I was watching Jeremy doing his classification, then he said what I consider the most interesting take away of this lesson, I don't remember the exact words but he said something like this:\n",
    "> You should train your model first, then clean the data later. Training the model on unclean data will show you where the model struggles and helps you to clean the data better and retrain later on a clean data to have better results.  \n",
    "\n",
    "Now That we trained our model on unclean data, we need to see where he did mess up the most. We can do that by using the following code:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c74c4-f23a-41e0-a442-e59063270b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(5, nrows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1666b8ae-0145-4863-a1b7-cd422b6f6415",
   "metadata": {},
   "source": [
    "fastai includes a handy GUI for data cleaning called `ImageClassifierCleaner` that allows you to choose a category and the training versus validation set and view the highest-loss images (in order), along with menus to allow images to be selected for removal or relabeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6bb5e7-9f31-4a74-8d7d-135cff73e271",
   "metadata": {},
   "source": [
    "![](cleaner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2f61e-b682-4779-9913-8022d23a1e0c",
   "metadata": {},
   "source": [
    "The problem I had with this dataset, was that many images were of bottles containing dry coriander/parsley or images containing coriander/parsley in their grain form. You must make sure that your dataset is absolutely representative of what you want to classify before proceeding to your final training.  \n",
    "\n",
    "Now that our dataset is clean, we retrain our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb5774-1352-47f4-8b04-05566927e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('plants')\n",
    "dls = plants.dataloaders(path)\n",
    "learn = vision_learner(dls, resnet18, metrics=error_rate)\n",
    "learn.fine_tune(4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268bc0ff-0e35-4eb2-a970-d13c01bc8504",
   "metadata": {},
   "source": [
    "Our model is now ready to use, let's export it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc371d-792d-45b2-91d3-7f87beafd991",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export('plants.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c569f97e-45ef-49af-8a19-cbaa14a1df0c",
   "metadata": {},
   "source": [
    "now we have our model in a Python Pickle File, it contains the architecture and weights of our model which will make it ready to be deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef7e41c-dd2c-4772-a896-c6111e9fe65a",
   "metadata": {},
   "source": [
    "# Step 2: Deploy your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5118888-e0f0-4e28-9613-05a4d8f3cd4e",
   "metadata": {},
   "source": [
    "For deploying our model we will be using HuggingFace Spaces and Gradio, you can follow this amazing [tutorial](https://www.tanishq.ai/blog/gradio_hf_spaces_tutorial).  \n",
    "\n",
    "If you prefer a more minimalist explanation, you can stick with me. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a3b16-26b6-42b9-81c2-2e3531b45a4e",
   "metadata": {},
   "source": [
    "## 2.1 Create a HuggingFace Account "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e61275-852e-4501-88d0-afdce12e3a7f",
   "metadata": {},
   "source": [
    "- Visit [HuggingFace](https://huggingface.co/) Website and Create an account\n",
    "- Now go to [HuggingFace Spaces](https://huggingface.co/spaces) and Create a New Space \n",
    "- In the SDK section select `Gradio` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c745f26-5926-4fc9-90dd-bac887538a53",
   "metadata": {},
   "source": [
    "![](gradio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378dd17c-6e26-4f46-8db0-f19b3665b0f5",
   "metadata": {},
   "source": [
    "You have now a new space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed24404a-d3b1-47cd-aad5-16b81c84ea2f",
   "metadata": {},
   "source": [
    "- Go to your terminal in install git lfs (to be able to upload large files) using the following command:  \n",
    "`git lfs install`\n",
    "- Now clone your space to your computer using the command:  \n",
    "`git clone https://huggingface.co/spaces/<your_username>/<your_spacename>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3549201f-7620-4434-b885-c42f223981c5",
   "metadata": {},
   "source": [
    "## 2.2 Creating a Gradio app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc4d7f-0eeb-49f0-b9e4-4d026562365a",
   "metadata": {},
   "source": [
    "Now we should create a Gradio app that takes an input from the user and queries the model for its prediction and return it.  \n",
    "Gradio usually requires a python file named `app.py` but we will be doing things differently.  \n",
    "In the fastai course we used JupyterNotebooks and `nbdev` to create such file, to do so, create a new Jupyter Notebook (in the folder of your cloned repo) and fill the first cell with this code:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28fab3-5aa8-49a9-87db-99ddbf0d74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<add a pipe character here>default_exp app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7aa46b-fa1d-4205-a0a4-31e2eb37d11a",
   "metadata": {},
   "source": [
    "`#|default_exp app`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a616b-c52d-4f51-a2d8-57ca586caa7f",
   "metadata": {},
   "source": [
    "Now let's import fastai vision and gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20003ae8-487c-4ef1-8027-c03e9464c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<add a pipe character here>export\n",
    "from fastai.vision.all import *\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46612fe-2472-413e-89a3-bdd15828eb9f",
   "metadata": {},
   "source": [
    "the first comment in the cell should be `#|default_exp app`, it is is an indicator for `nbdev` to export the file and name it \"app\". The rest is imports we will need. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4fb72f-61f0-4cce-95b8-6794581231a5",
   "metadata": {},
   "source": [
    "Now move the exported `model.pkl` model to the folder of your repo and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa30b386-a5b2-4417-8641-e11877138e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<add a pipe character here>export\n",
    "learn = load_learner('model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b867b1-0a0f-44b9-a209-86301236ee88",
   "metadata": {},
   "source": [
    "that `#|export` line indicates to `nbdev` that the code of this cell needs to be included in the final exported app.py, if you want to do some tests for example import some images and try the model on them you can do so on cells that do not have the comment on their first line.  \n",
    "Our model is now up and ready, let's create a Gradio interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89efa68-d8fb-4e15-b3ad-6daa67dc89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<add a pipe character here>export\n",
    "categories = ('coriander', 'parsley')\n",
    "def classify_image(img):\n",
    "    pred, idx, probs = learn.predict(img)\n",
    "    return dict(zip(categories,map(float,probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0636a88-da30-404b-813f-b70b81cb8d6d",
   "metadata": {},
   "source": [
    "Let's break this down a bit:  \n",
    "- `categories` contains the categories that our classifier chooses from.  \n",
    "\n",
    "Gradio requires us to give him a function that he will call, this function is `classify_image`, it takes an image as an arguments.\n",
    "the `learner` has a method predict that takes an input and returns 3 things:  \n",
    "-  `pred`: `True` for positive class, `False` for negative class  \n",
    "-  `idx`: gives the index of the class  \n",
    "-  `probs`: gives the probability that this item is in this class for example if we have two categories probs will be a (2,1) array with probs[0] the probability of the item being in class 0, and probs[1] the probability of it being in class 1.      \n",
    "\n",
    "The function returns a dictionary with `categories` as keys and `float(probs)` as values, if you are not familiar with the `dict(zip())` paradigm, check up [this video](https://www.youtube.com/watch?v=dBgxirJG3K8&ab_channel=Rodrigo%F0%9F%90%8D%F0%9F%93%9D). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1637a5dd-cd55-4e8e-9a83-f1e795543223",
   "metadata": {},
   "source": [
    "Now that we have our function set, let's go and create the actual interface:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de72627-9e97-4257-b19c-aa5ac1ad915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<add a pipe character here>export\n",
    "image = gr.inputs.Image(shape=(192,192))\n",
    "label = gr.outputs.Label()\n",
    "examples = ['coriander.jpg','parsley.jpg']\n",
    "\n",
    "intf = gr.Interface(fn=classify_image, inputs = image, outputs = label, examples = examples)\n",
    "intf.launch(inline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7710b1-943b-4ab6-8adf-bc525c83e360",
   "metadata": {},
   "source": [
    "We created an `image` input object and a `label` output object. We also added two images to our main folder of actual coriander and parsley respectively named `coriander.jpg` and `parsley.jpg`.  \n",
    "\n",
    "Then we proceed to creating an interface that takes as parameters:  \n",
    "- `fn`: the function `classify_image`  \n",
    "- `inputs`: the input image  \n",
    "- `outputs`: the output label  \n",
    "- `examples`: this is optional, use it if you want to add examples to help the user.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e0f78-7d08-4d59-ab2b-f7c97adbb878",
   "metadata": {},
   "source": [
    "Our notebook is ready to be converted into a Gradio app! We will use `nbdev` to do so add this cell to the bottom of your notebook and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a147aeb-94c1-4d8e-9ade-7a87307f63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev\n",
    "nbdev.export.nb_export('app.ipynb', 'app')\n",
    "print('Export successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cac0a1-a021-4075-81b7-f8712f691a88",
   "metadata": {},
   "source": [
    "We are almost done, all we need to do now is to create a requirements.txt file and fill it with the following:  \n",
    "``` \n",
    "fastai  \n",
    "torch  \n",
    "gradio  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7edfa4-d9c2-402f-ab8c-c67d234a455b",
   "metadata": {},
   "source": [
    "Everything is good now, let's push this to HuggingFace:  \n",
    "You can either do this from the terminal (reminder: make sure to install git lfs) using the following commands:  \n",
    "` git add .`  \n",
    "` git commit -m \"add your custom message\"`  \n",
    "` git push`  \n",
    "Or via the HuggingFace Website, Go to your space -> Files and versions. And add your files manually from there.  \n",
    "\n",
    "When everything is done, visit `https://huggingface.co/spaces/<your_username>/<your_spacename>` and you will have a fully operational coriander and parsley classifying app that you can share with your friends, it's not perfect but still it's better than wild guessing.  \n",
    "\n",
    "Here is [mine](https://huggingface.co/spaces/Kiriyama0/corvspar)  \n",
    "\n",
    "Note: Hugging Face Spaces that run on the default cpu-basic hardware (free tier), will go to sleep if inactive for more than a set time (currently, 72 hours). Anyone visiting your Space will restart it automatically. When it goes to sleep you can restart it by going to your spaces.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22476fde-5c86-4425-b6c4-e861a26ac5e2",
   "metadata": {},
   "source": [
    "![](hf.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
