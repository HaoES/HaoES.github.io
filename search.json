[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hamza ES-SAMAALI",
    "section": "",
    "text": "I am currently a Data Science Professor at EMSI - Marrakech. I love learning as much as I love teaching. This blog is where I will be sharing my thoughts about Machine Learning related topics."
  },
  {
    "objectID": "posts/Why-this-blog/index.html",
    "href": "posts/Why-this-blog/index.html",
    "title": "Why Blogging? (and How?)",
    "section": "",
    "text": "I’ve had a complex relationship with blogging. I’ve always wanted to blog but found it too much of a hassle.\nThen I stumbled upon this post and decided to start my own blog documenting my journey as a Machine Learning enthusiast.\nThis blog will principally be a way for me to digest what I’ve learned (the best way of learning is teaching), share my thoughts and the projects I do.\nI had two main choices for blogging, the first is Medium and the second is through Github Pages.\nFor no explainable reason I never liked Medium maybe because of the annoying reading limit it has when it asks you to connect (I hold grudges easily). As for Github Pages, I found Jekyll too much of a pain to deal with, too lazy to learn its intricacies.\nFortunately this year I decided to start the famous fast.ai course and Jeremy kept talking about Quarto and how it generates blog posts from Jupyter Notebooks. I also stumbled upon this blog of a fellow fast.ai student and used his posts to create my own blog.\nSo let me share with you How I did it:"
  },
  {
    "objectID": "posts/Why-this-blog/index.html#installing-quarto-in-wsl",
    "href": "posts/Why-this-blog/index.html#installing-quarto-in-wsl",
    "title": "Why Blogging? (and How?)",
    "section": "Installing Quarto in WSL",
    "text": "Installing Quarto in WSL\nto install Quarto in WSL (or Ubuntu) via nbdev use:\nmamba install -c fastchan nbdev  \nnbdev_install_quarto"
  },
  {
    "objectID": "posts/understanding-stable-diffusion/index.html",
    "href": "posts/understanding-stable-diffusion/index.html",
    "title": "Understanding Stable Diffusion",
    "section": "",
    "text": "In this blog post I will be presenting a high level explanation of what Stable Diffusion is and how it works. This is the insights I’ve got from Lesson 9 of the fast.ai 2022 part 2 course.\nAccording to Wikipedia Stable Diffusion is a deep learning, text-to-image model released in 2022. It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt. It was developed by researchers from the CompVis Group at Ludwig Maximilian University of Munich and Runway with a compute donation by Stability AI and training data from non-profit organizations."
  },
  {
    "objectID": "posts/understanding-stable-diffusion/index.html#unet",
    "href": "posts/understanding-stable-diffusion/index.html#unet",
    "title": "Understanding Stable Diffusion",
    "section": "1 - UNET:",
    "text": "1 - UNET:\n\n\n\nLet’s examine the input and output of the Unet model and explore methods to accelerate the training process through compression:\n\nInput: The input to the Unet is a somewhat noisy image, ranging from minimally noisy to completely noisy.\nOutput: The model’s output aims to represent the noise pattern present in the input image.\n\nBy subtracting the output noise from the input image, we obtain an approximation of the denoised image.\nCurrently, the handwritten input images are 28x28 in size, but the desired goal is to generate larger images. The Unet models typically work with 512x512x3 images for which millions of noisy versions are used during training. However, training such a model using these large datasets can be time-consuming.\nTo expedite the training process, we can leverage an insightful approach that capitalizes on the fact that pixel values exhibit minimal local variation. Storing each individual pixel value is unnecessary. Instead, we can utilize compression techniques like JPEG, which significantly reduce the amount of storage required to represent an image while retaining essential information. This compression strategy allows us to achieve faster training without compromising the overall image quality."
  },
  {
    "objectID": "posts/understanding-stable-diffusion/index.html#vae---variational-autoencoders",
    "href": "posts/understanding-stable-diffusion/index.html#vae---variational-autoencoders",
    "title": "Understanding Stable Diffusion",
    "section": "2- VAE - Variational autoencoders:",
    "text": "2- VAE - Variational autoencoders:\nNow, let’s explore the process of compressing it using an autoencoder (AE). The autoencoder’s architecture involves progressively halving the number of pixels per dimension and doubling the number of channels at each level using stride two convolutions. Finally, we incorporate a few ResNet-like blocks to further reduce the channel count from 24 to 4.\n\n\n\nWe began with a 512x512x3 image and successfully obtained its compressed representation, known as “latents,” which now has a size of 64x64x4. The compression factor achieved is an impressive 48, resulting in a much smaller representation. This encoding process, which transforms the larger image into a compact form, is carried out by the encoder.\nThe chosen compression factor becomes meaningful based on how effectively we can reconstruct the original image from these 64x64x4 latents. To achieve this, we will develop the inverse process, referred to as the decoder. Once both the encoder and decoder are constructed, they can be combined, and the entire autoencoder can be trained effectively.\nIn summary, the process involves encoding the “big” image into smaller latents (encoder), followed by decoding those latents back to reconstruct the original image (decoder), and finally, training the complete autoencoder with this setup.\n\n\n\nWe can use MSE and train this, in the beginning we will get random outputs but later we should get close to our input\n\n\n\nSo what is the point of a model that spits back an output that is identical to the input?\n\n\n\nThe encoder, depicted in green, is responsible for transforming a larger image into a compact representation. Conversely, the decoder, shown in red, performs the inverse operation, reconstructing the original image from the compressed representation. If I wish to share an image with someone, I can pass it through the encoder, resulting in a representation that is 48 times smaller than the original picture. The recipient, possessing a copy of the trained decoder, can then use it to reverse the process and recover the original image. Essentially, this entire mechanism functions as a compression algorithm, facilitating efficient image transmission and reconstruction.\nTo utilize the compression algorithm effectively, we employ the Unet by passing the compressed “smaller” latents instead of the original “bigger” images as inputs.\nThe updated inputs and outputs of the Unet are as follows: - Input: Latents with some level of noise - Output: Noise\nBy subtracting the output from the input, we obtain denoised latents, which are then fed into the decoder of the autoencoder to generate the best approximation of the denoised image. This autoencoder is called a Variational Autoencoder (VAE).\nIn summary, the process involves starting with a 512x512x3 image, passing it through the VAE’s encoder to obtain compressed latents of size 64x64x4. Subsequently, these latents are passed through the Unet to predict the noise. The noise is then subtracted from the encoder’s latents, resulting in denoised latents. These denoised latents are finally passed through the decoder of the VAE to generate a 512x512x3 image.\nImportant points to consider:\n\nThe VAE serves as an optional building block, allowing faster training of the Unet with smaller-sized latents rather than full images.\nDuring inference, the encoder of the VAE is not required; it is only necessary during the training phase."
  },
  {
    "objectID": "posts/understanding-stable-diffusion/index.html#clip",
    "href": "posts/understanding-stable-diffusion/index.html#clip",
    "title": "Understanding Stable Diffusion",
    "section": "3 - CLIP:",
    "text": "3 - CLIP:\nNow, let’s explore the significance of text prompts in the process. Instead of merely inputting noise and receiving a digit in return, can we experiment with instructing the system to generate a particular number, for instance, “3”?\n\n\n\nTo accomplish this, besides providing the noisy input image, we will also include a one-hot encoded representation of the digit “3”.\n\n\n\nCurrently, we are inputting two elements into this model: the image pixels and the one-hot encoded vector representing the digit it corresponds to. Consequently, the model will learn to predict the noise, benefiting from the extra information about the original digit. This improvement is expected compared to the previous model’s noise prediction capability.\nOnce the model is trained, feeding it the one-hot encoded vector for “3” along with the noise will enable it to recognize the noise that does not represent the number three. This process is referred to as “guidance,” as it helps the model generate the desired image.\nHowever, one might wonder if one-hot encoded vectors are the most efficient approach. For instance, if we wish to create an image from the phrase “a cute teddy,” using one-hot encoded vectors for every phrase could be highly inefficient.\nTo address this, we can develop a model capable of taking the phrase “a cute teddy” as input and generating a vector of numbers, known as embeddings, which somehow represents the characteristics of “cute teddies.”\nIn practice, we can obtain images from the internet, where those with alt tags will have associated text descriptions. These descriptions can be leveraged to create meaningful input for the model’s embeddings, enabling it to understand and generate images that align with the given text description.\n\n\n\nNow we can create two models, one model which is a text encoder and one model which is an image encoder.\n\n\n\nTo achieve our goal, we have two encoders—one for processing images and another for handling text. Each encoder produces two embeddings.\nWhen we pass the image of the swan through the image model, our aim is to obtain embeddings that closely resemble the ones generated by passing the text “the graceful swan” through the text encoder. In essence, we desire similarity between these embeddings. To accomplish this, we leverage the dot product as a measure of similarity. A higher dot product indicates a greater degree of similarity between the embeddings.\n\n\n\nNow, we have a grid containing images and corresponding text, and by computing the dot product of their embeddings, we obtain a score for each combination. Our objective is to achieve high dot product scores (represented by blue diagonal elements) only for matching image-text pairs. Conversely, for non-matching pairs of text and image, we aim to obtain lower dot product scores (depicted by red off-diagonal elements).\n\n\n\nSo our loss function can be defined as adding all the diagonal elements and subtracting from it the off-diagonal elements.\n\n\n\nIf we want this loss function to be good then we’re going to need the weights of our model for the text encoder to spit out embeddings that are very similar to the image embeddings that they’re paired with and not similar to the embedding of the images they are not paired with.\nNow we can feed our text encoder with “a graceful swan”, “some beautiful swan”, “such a lovely swan” and these should all give very similar embeddings because these would all represent very similar images of swans.\nWe’ve successfully created two models that put text and images into the same space, a multimodal(using more than one mode-images and text) model.\nSo we took this detour because creating 1-hot encoded vectors for all the possible phrases was impractical. We can can now take our phrase - “a cute teddy bear” and feed it in text encoder to get out some features/embeddings.\n\n\n\nInstead of using 1-hot encoded vectors as guides during Unet training, we utilize the features generated by the text and image encoders. So, when we input the phrase “a cute teddy” into the text encoder, it produces embeddings that serve as guidance for our model to transform the input noisy image into something resembling “cute teddies” it has encountered before.\nThis pair of models is known as CLIP, which stands for Contrastive Language-Image Pre-training, and the loss function employed is called contrastive loss.\nLet’s now review the building blocks we have established so far.\n\n\n\n\nwe’ve got a Unet that can denoise latents into unnoisy latents\nwe’ve got the decoder of VAE that can take latents and create an image\nwe’ve got the CLIP text encoder which can guide the Unet with captions\n\nStable diffusion is a latent diffusion model and what that means is that it doesn’t operate in the pixel space, it operates on in the latent space of some other autoencoder model and in this case that is a variational autoencoder."
  },
  {
    "objectID": "posts/understanding-stable-diffusion/index.html#additional-things",
    "href": "posts/understanding-stable-diffusion/index.html#additional-things",
    "title": "Understanding Stable Diffusion",
    "section": "4 - Additional Things:",
    "text": "4 - Additional Things:\nThere are two more things Jeremy talks about: - Score function - time-steps\n\n\n\nThese gradients are commonly referred to as the “score function.”\nThe term “time steps” is frequently found in various papers, although we did not employ any actual time steps during our training process. This terminology originated from the initial mathematical formulations. Despite this, we can understand the concept of time steps without it being directly related to real-world time.\nDuring training, we introduced diverse levels of noise to our images, varying from highly noisy to noise-free and even pure noise.\nTo establish a noising schedule, we utilize a monotonically decreasing function. Let’s consider the x-axis (“t”) ranging from one to a thousand. We randomly select a number within this range and use the noise schedule to determine the corresponding sigma (also referred to as beta in some papers). For instance, if we randomly choose the number four, we would look up the corresponding sigma value on the y-axis, representing the amount of noise to add to the image with this randomly chosen value of four.\n\n\n\nDuring the training process, the level of noise added to each image varies depending on a random selection. If a value close to one is chosen, the resulting image will have a substantial amount of noise, whereas a value near one thousand will lead to minimal noise.\nTo achieve this randomness in training, we need to determine a random amount of noise for every image. This can be achieved by selecting a random number between one and a thousand and using a noise scheduler function to obtain the corresponding sigma value for the noise to be added.\nPreviously, people often referred to this random number as the “time step,” but nowadays, the noise scheduler lookup is becoming less prevalent. Many practitioners now simply state the magnitude of noise used.\nDuring the training process, for each mini-batch, a random batch of images is selected from the training set. Additionally, either a random amount of noise or a random “t” value (which is then used to find the amount of noise from the noise scheduler) is chosen. These noisy images are then passed into the model for training, allowing the model’s weights to learn how to predict noise effectively.\n\n\n\nHow do we precisely carry out this inference process? When generating a picture from pure noise, it corresponds to t=0 on the noise scheduler, representing maximum noise. The objective is to teach the model to eliminate noise effectively. However, attempting to accomplish this in a single step could result in poor-quality images.\n\n\n\nIn practice, the noise prediction is multiplied by a constant “c,” akin to a learning rate, but here it is not updating model weights; rather, it updates individual pixels. This updated prediction is then subtracted from the original noisy pixels, resulting in a slightly denoised image.\nHowever, the model does not immediately reach the final denoised image because certain image characteristics, such as those appearing with t=1 (which correspond to poor-quality images), were not encountered in the training set. Consequently, the model lacks knowledge on how to handle such images. To address this, only a small factor of the noise is subtracted, ensuring that the process repeats with somewhat noisy latents, allowing the model to iterate and refine the denoising gradually.\nDeciding the appropriate value for “c” and determining how to perform the subtraction from the noise prediction are vital aspects addressed in the actual diffusion sampler. The sampler manages both the addition of noise and the subsequent subtraction during the diffusion process.\nInterestingly, this approach bears resemblance to deep learning optimizers. For instance, momentum in optimizers suggests increasing the change in parameters when they are changed repeatedly by a similar amount over multiple steps. Similarly, Adam optimizer considers changes in variance. Although diffusion-based models and optimizers stem from different mathematical domains (differential equations versus optimization), they share parallel concepts. Differential equation solvers also focus on taking more significant steps to converge quickly. They often utilize t as an input, a common feature in most diffusion models, although we have yet to discuss it in detail.\n\n\n\nIn diffusion models, it’s a common practice to include not only the input pixels and captions but also the parameter “t.” The intention behind incorporating “t” is to provide the model with information about the amount of noise present, as “t” is associated with the noise level.\nHowever, Jeremy strongly suspects that this premise might be incorrect because for a sophisticated neural network, determining the noise level can be relatively straightforward. Consequently, when the need for passing “t” diminishes, the problem transforms from one resembling differential equations to that resembling optimizers.\nBy swapping Mean Squared Error (MSE) with perceptual loss, various possibilities emerge, expanding the approach to be viewed as an optimization problem rather than merely a differential equation-solving problem. This shift in perspective unlocks new avenues for exploration and potential improvements in the model’s performance."
  },
  {
    "objectID": "posts/breaking_down_backpropagation_code/index.html",
    "href": "posts/breaking_down_backpropagation_code/index.html",
    "title": "Breaking down backpropagation implementation",
    "section": "",
    "text": "I have always had this bad habit while learning where I would just take the informations in passively and not try to apply any of it, even while studying math I only read the theorems and never do exercises.\nThis is a very bad habit that I am trying to get rid of lately while doing the fastai course. I try my best not to let anything slide without explicitly understanding it.\nOr that’s what I thought. Because recently while browsing the fastai forum I stumbled upon a question the code of backpropagation. A concept that I thought I understood very well. The question was and I quote:\n\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp,self.targ = inp,targ\n        self.out = mse(inp, targ)\n        return self.out \n\n    def backward(self):\n        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]\n\n\nclass Lin():\n\n    def __init__(self, w, b): self.w,self.b = w,b\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = lin(inp, self.w, self.b)\n        return self.out\n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n\nShouldn’t it be self.out.g instead of self.inp.g in the backward definition of Mse class. I don’t know how Lin backward() automatically gets self.out.g value. Can some one explain?\nEnd of Quote\nWhen I read this question I realized that I didn’t have the answer so I revisited the video for the lesson 13 and came up with a “detailed” response. Here it is:\nTo understand this let us first set all the code we need, then we will take it execute it step by step. Our building blocks are: - the lin function: def lin(x, w, b): return x@w + b - the mse function: def mse(output,target): return ((output.squeeze()-target)**2).mean() and the classes: Mse(), ReLU(), Lin() and Model() Now to create our model and compute backpropagation we run the following code:\nmodel = Model(w1, b1, w2, b2)\nwhat happens now?: We are calling the Model constructor so if we look inside the object model we will find:\nmodel.layers = [Lin(w1,b1),Relu(),Lin(w2,b2)]\nmodel.loss = Mse() \nLet’s name our layers L1, R, and L2 to make the explanation easier to follow. so L1.w = w1, L1.b = b1, L2.w = w2 and L2.b = b2.\nNow let’s execute the following line:\nloss = model(x_train, y_train)\nhere we are using the model object as if it was a function, this will trigger the __call__ method, here is the code for it:\n def __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\nlet’s execute it: in our case x = x_train and targ = y_train now let’s go through that for loop: for l in self.layers: x = l(x) the contents of model.layers is [L1,R,L2] so the first instruction will be: x = L1(x) similarly here again we are using L1 as function so let’s go see what’s in its __call__ method and run it:\n# Lin Call method\ndef __call__(self, inp):\n        self.inp = inp\n        self.out = lin(inp, self.w, self.b)\n        return self.out\nso we are assigning inp to self.inp, in this case L1.inp = x_train and L1.out = lin(inp, w1,b1) = x_train @ w1 + b1.\nThe call method returns self.out so the new value of x will be x = L1.out.\nThe first iteration of the loop is done, next element is the layer R, so x = R(x)\n# ReLU call method\ndef __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\nso now we have R.inp = L1.out R.out = relu(L1.inp) # basically equal to L1.inp when it's > 0, 0 otherwise.\nNow the new value of x is x = relu(L1.inp) The second iteration is done, next element is the layer L2, so x = L2(x) now we have L2.inp = relu(L1.inp) and L2.out = relu(L1.inp) @ w2 + b2.\nThe new value of x is x = L2.out = relu(L1.inp) @ w2 + b2.\nThe for loop has ended. Let’s go to the next line of code return self.loss(x, targ) We saw earlier that model.loss = Mse() so we are using the __call__ method of the Mse class:\n# call method of the Mse class\ndef __call__(self, inp, targ):\n        self.inp, self.targ = inp, targ\n        self.out = mse(inp, targ)\n        return self.out\nnow we have mse.inp = x, mse.targ = targ and mse.out = mse(x, targ) = ((x.squeeze()-targ)**2).mean().\nThe method return mse.out so loss = mse.out.\nFinally we get to the part which confused us both\nmodel.backward()\nit calls the backward method of the Model class:\n# backward method of the Model class\ndef backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\nIn the first line we have model.loss.backward() which is none other than the backward method of the Mse class. because remember that loss is an instance of the Mse class.\n# backward method of Mse\ndef backward(self):\n        self.inp.g = 2 * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.inp.shape[0]\nSo here we compute mse.inp.g and we saw earlier that mse.ing = x so we are in fact computing x.g and it’s equal to x.g = 2 * (x.squeeze() - targ).unsqueeze(-1) / x.shape[0]\nx as you know is the output of our MLP (multi level perceptron), and the gradient of the loss with respect to the output is stored in the output tensor i.e x.g. So that’s why it should be indeed inp.g and not out.g in the backward method of the Mse class.\nNow in order to find out how backward of Lin get the out.g value let’s continue executing our code. We have have executed the first line now let’s run the for loop:\n for l in reversed(self.layers): l.backward()\nthe first value of l is L2 (because we are going through the reversed list of layers) so let’s run L2.backward()\n# Lin backward\ndef backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\nWe already know that:\nL2.inp = relu(L1.inp)\nL2.out = relu(L1.inp) @ w2 + b2 = x\nso when we call L2.backward() this method will perform the following updates:\nL2.inp.g =  L2.out.g @ L2.w.t() # which is equivalent to L2.inp.g = x.g @ w2.t() \nw2.g = L2.inp.t() @ L2.out.g\nb2.g = L2.out.g.sum(0)\nAs you can see Lin knows automatically what out.g is, because when we ran model.loss.backward() we calculated it. So now we have computed L2.inp.g (which is R.out.g) ,w2.g and b2.g.\nThe first iteration of the loop has ended, next l=R and we will run R.backward:\ndef backward(self): self.inp.g = (self.inp>0).float() * self.out.g\nWe know that R.inp = L1.out and R.out = relu(L1.inp) The following updates will occur:\nR.inp.g = (R.inp > 0).float() * R.out.g \nNow we have computed R.inp.g (which is L1.out.g).\nThis iteration is done, next is l = L1 so we will call L1.backward().\nWe know that L1.inp = x_train and that L1.out  = R.inp So calling backward of L1 will give us the following updates:\nL1.inp.g =  L1.out.g @ w1.t() # which is equivalent to L1.inp.g = R.inp.g @ w1.t() \nw1.g = L1.inp.t() @ L1.out.g\nb1.g = L1.out.g.sum(0)\nThat’s it.\nThe main takeaway is that backpropagation strats at the end and compute the gradient of the loss and stores it in the output tensor of the neural network (which is the input tensor of the loss function, and that’s what’s confusing)."
  },
  {
    "objectID": "posts/coriander-parsley-classifier/index.html",
    "href": "posts/coriander-parsley-classifier/index.html",
    "title": "A coriander قزبر vs معدنوس Parsley Classifier",
    "section": "",
    "text": "Coriander قزبر and parsley معدنوس are two staple ingredients of Moroccan cuisine, our moms have the superpower of easily distinguishing between them, but us mortals cannot.\nIn lesson 2 of Deep Learning for Coders 2022 part 1, Jeremy presented a simple image classification model for grizzly, black and teddy bears. I will be using a similar model to help us on this strenuous and confusing task."
  },
  {
    "objectID": "posts/coriander-parsley-classifier/index.html#create-a-huggingface-account",
    "href": "posts/coriander-parsley-classifier/index.html#create-a-huggingface-account",
    "title": "A coriander قزبر vs معدنوس Parsley Classifier",
    "section": "2.1 Create a HuggingFace Account",
    "text": "2.1 Create a HuggingFace Account\n\nVisit HuggingFace Website and Create an account\nNow go to HuggingFace Spaces and Create a New Space\nIn the SDK section select Gradio\n\n\nYou have now a new space.\n\nGo to your terminal in install git lfs (to be able to upload large files) using the following command:\ngit lfs install\nNow clone your space to your computer using the command:\ngit clone https://huggingface.co/spaces/<your_username>/<your_spacename>"
  },
  {
    "objectID": "posts/coriander-parsley-classifier/index.html#creating-a-gradio-app",
    "href": "posts/coriander-parsley-classifier/index.html#creating-a-gradio-app",
    "title": "A coriander قزبر vs معدنوس Parsley Classifier",
    "section": "2.2 Creating a Gradio app",
    "text": "2.2 Creating a Gradio app\nNow we should create a Gradio app that takes an input from the user and queries the model for its prediction and return it.\nGradio usually requires a python file named app.py but we will be doing things differently.\nIn the fastai course we used JupyterNotebooks and nbdev to create such file, to do so, create a new Jupyter Notebook (in the folder of your cloned repo) and fill the first cell with this code:\n\n#<add a pipe character here>default_exp app\n\n#|default_exp app\nNow let’s import fastai vision and gradio\n\n#<add a pipe character here>export\nfrom fastai.vision.all import *\nimport gradio as gr\n\nthe first comment in the cell should be #|default_exp app, it is is an indicator for nbdev to export the file and name it “app”. The rest is imports we will need.\nNow move the exported model.pkl model to the folder of your repo and load it.\n\n#<add a pipe character here>export\nlearn = load_learner('model.pkl')\n\nthat #|export line indicates to nbdev that the code of this cell needs to be included in the final exported app.py, if you want to do some tests for example import some images and try the model on them you can do so on cells that do not have the comment on their first line.\nOur model is now up and ready, let’s create a Gradio interface:\n\n#<add a pipe character here>export\ncategories = ('coriander', 'parsley')\ndef classify_image(img):\n    pred, idx, probs = learn.predict(img)\n    return dict(zip(categories,map(float,probs)))\n\nLet’s break this down a bit:\n- categories contains the categories that our classifier chooses from.\nGradio requires us to give him a function that he will call, this function is classify_image, it takes an image as an arguments. the learner has a method predict that takes an input and returns 3 things:\n- pred: True for positive class, False for negative class\n- idx: gives the index of the class\n- probs: gives the probability that this item is in this class for example if we have two categories probs will be a (2,1) array with probs[0] the probability of the item being in class 0, and probs[1] the probability of it being in class 1.\nThe function returns a dictionary with categories as keys and float(probs) as values, if you are not familiar with the dict(zip()) paradigm, check up this video.\nNow that we have our function set, let’s go and create the actual interface:\n\n#<add a pipe character here>export\nimage = gr.inputs.Image(shape=(192,192))\nlabel = gr.outputs.Label()\nexamples = ['coriander.jpg','parsley.jpg']\n\nintf = gr.Interface(fn=classify_image, inputs = image, outputs = label, examples = examples)\nintf.launch(inline=False)\n\nWe created an image input object and a label output object. We also added two images to our main folder of actual coriander and parsley respectively named coriander.jpg and parsley.jpg.\nThen we proceed to creating an interface that takes as parameters:\n- fn: the function classify_image\n- inputs: the input image\n- outputs: the output label\n- examples: this is optional, use it if you want to add examples to help the user.\nOur notebook is ready to be converted into a Gradio app! We will use nbdev to do so add this cell to the bottom of your notebook and run it:\n\nimport nbdev\nnbdev.export.nb_export('app.ipynb', 'app')\nprint('Export successful')\n\nWe are almost done, all we need to do now is to create a requirements.txt file and fill it with the following:\nfastai  \ntorch  \ngradio  \nEverything is good now, let’s push this to HuggingFace:\nYou can either do this from the terminal (reminder: make sure to install git lfs) using the following commands:\ngit add .\ngit commit -m \"add your custom message\"\ngit push\nOr via the HuggingFace Website, Go to your space -> Files and versions. And add your files manually from there.\nWhen everything is done, visit https://huggingface.co/spaces/<your_username>/<your_spacename> and you will have a fully operational coriander and parsley classifying app that you can share with your friends, it’s not perfect but still it’s better than wild guessing.\nHere is mine\nNote: Hugging Face Spaces that run on the default cpu-basic hardware (free tier), will go to sleep if inactive for more than a set time (currently, 72 hours). Anyone visiting your Space will restart it automatically. When it goes to sleep you can restart it by going to your spaces."
  },
  {
    "objectID": "posts/understanding-gradient-descent/index.html",
    "href": "posts/understanding-gradient-descent/index.html",
    "title": "Understanding Gradient Descent",
    "section": "",
    "text": "As you know I am currently doing fast.ai course. And I decided to take my time doing Lesson 3 because it’s about a very important topic: “Gradient Descent”.\nI’ve done many ML courses by the past (Andrew Ng’s Course, Aurelien Geron’s book, etc…) and each of these helped me understand what gradient descent was to a certain extent. But now I can confidently say that I’ve finally grasped what Gradient Descent is truly about.\nYou too can achieve the same understanding by following the steps I did:\n\nWatch the lesson 3 video and follow with Jeremy\nRead Chapter 4 of the fastai book\nWatch Andrej Karpathy’s amazing video: The spelled-out intro to neural networks and backpropagation: building micrograd\n\nI can’t stress enough how amazing Andrej’s video is!!! It’s a perfect complementary material to fast.ai’s lesson 3, and he goes step by step on how to build micrograd which is an autograd engine.\nDon’t forget to write the code along with both Jeremy and Andrej.\nDo this and you will have understood how Gradient Descent works and you will know how to implement it too."
  },
  {
    "objectID": "posts/fastai-deep-learning-for-coders-part-1/index.html",
    "href": "posts/fastai-deep-learning-for-coders-part-1/index.html",
    "title": "Summing up fastai’s Deep Learning for Coders Part 1",
    "section": "",
    "text": "In the ever-evolving world of artificial intelligence, deep learning has emerged as a powerful tool for solving complex problems. However, getting started with deep learning can be a daunting task, especially for coders without prior experience in this field. Fast.ai’s Practical Deep Learning for Coders course, offers an accessible and hands-on approach to learning deep learning concepts. In this blog post, we’ll explore the first part of the course, highlighting its key elements and sharing the invaluable insights gained from this immersive learning experience.\n\nA Practical Approach to Learning Deep Learning:\nFast.ai’s approach to teaching deep learning is unique. Instead of diving straight into the theory and mathematics behind neural networks, the course focuses on hands-on coding exercises and real-world applications. This approach enables coders to build and deploy their own deep learning models from day one, fostering a deeper understanding of the underlying concepts.\n\n\nFastai Library: Simplifying Deep Learning:\nAt the core of the course is the fastai library, a high-level deep learning framework built on top of PyTorch. This library abstracts away much of the complexity associated with training neural networks, making it easier for coders to experiment with different architectures and techniques. With its comprehensive set of pre-built functions, the fastai library allows learners to rapidly prototype and iterate their models.\n\n\nImage Classification and Data Augmentation:\nThe first part of the course delves into image classification, a fundamental problem in computer vision. Through a series of engaging lessons and coding exercises, learners are introduced to concepts such as convolutional neural networks (CNNs) and transfer learning. Moreover, the course emphasizes the importance of data augmentation techniques, enabling learners to improve model performance by generating synthetic training data. Jeremy also gives a plethora of advises regarding both the training and data augmentation phases.\n\n\nCollaborative Filtering and Natural Language Processing:\nBeyond computer vision, the course also covers collaborative filtering and natural language processing (NLP). With collaborative filtering, learners gain insights into recommender systems, enabling them to build personalized recommendation engines. In the NLP section, participants learn how to process and analyze text data using techniques like tokenization, language modeling, and sentiment analysis. These topics broaden the scope of the course, demonstrating the versatility of deep learning in various domains.\n\n\nLive Coding Session:\nDo not forget the “hidden” and most important part in my opinion of this amazing course: The live coding sessions. Jeremy walks you through how he went through a kaggle competition to get to the first place while explaining how to setup a proper working environment for kaggle competition and deep learning in general. YOU SHOULD NOT MISS THIS!!!\n\n\nConclusion:\nFast.ai’s Practical Deep Learning for Coders - Part 1 offers a remarkable entry point into the world of deep learning for coders. Through its hands-on approach, practical examples, and emphasis on real-world applications, the course equips learners with the necessary skills to build and deploy deep learning models. By leveraging the power of the fastai library and gaining a deep understanding of fundamental concepts, participants develop the confidence to explore and experiment with various domains within deep learning. If you’re a coder interested in venturing into the exciting realm of deep learning, Fast.ai’s course is undoubtedly an excellent starting point on your journey."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hamza's Blog",
    "section": "",
    "text": "Breaking down backpropagation implementation\n\n\n\n\n\n\n\nfastai\n\n\nml\n\n\nbackpropagation\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2023\n\n\nHamza ES-SAMAALI\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Stable Diffusion\n\n\n\n\n\n\n\nfastai\n\n\nml\n\n\nstable diffusion\n\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2023\n\n\nHamza ES-SAMAALI\n\n\n\n\n\n\n  \n\n\n\n\nSumming up fastai’s Deep Learning for Coders Part 1\n\n\n\n\n\n\n\nfastai\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2023\n\n\nHamza ES-SAMAALI\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Gradient Descent\n\n\n\n\n\n\n\ngradient\n\n\ndescent\n\n\nneural\n\n\nnetwork\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\nHamza ES-SAMAALI\n\n\n\n\n\n\n  \n\n\n\n\nA coriander قزبر vs معدنوس Parsley Classifier\n\n\n\n\n\n\n\nfastai\n\n\nml\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nHamza\n\n\n\n\n\n\n  \n\n\n\n\nWhy Blogging? (and How?)\n\n\n\n\n\n\n\nblogging\n\n\nquarto\n\n\njupyter\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\nHamza Es-samaali\n\n\n\n\n\n\nNo matching items"
  }
]