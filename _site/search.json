[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hamza ES-SAMAALI",
    "section": "",
    "text": "I am currently a Data Science Professor at EMSI - Marrakech. I love learning as much as I love teaching. This blog is where I will be sharing my thoughts about Machine Learning related topics."
  },
  {
    "objectID": "posts/Why-this-blog/index.html",
    "href": "posts/Why-this-blog/index.html",
    "title": "Why Blogging? (and How?)",
    "section": "",
    "text": "I’ve had a complex relationship with blogging. I’ve always wanted to blog but found it too much of a hassle.\nThen I stumbled upon this post and decided to start my own blog documenting my journey as a Machine Learning enthusiast.\nThis blog will principally be a way for me to digest what I’ve learned (the best way of learning is teaching), share my thoughts and the projects I do.\nI had two main choices for blogging, the first is Medium and the second is through Github Pages.\nFor no explainable reason I never liked Medium maybe because of the annoying reading limit it has when it asks you to connect (I hold grudges easily). As for Github Pages, I found Jekyll too much of a pain to deal with, too lazy to learn its intricacies.\nFortunately this year I decided to start the famous fast.ai course and Jeremy kept talking about Quarto and how it generates blog posts from Jupyter Notebooks. I also stumbled upon this blog of a fellow fast.ai student and used his posts to create my own blog.\nSo let me share with you How I did it:"
  },
  {
    "objectID": "posts/Why-this-blog/index.html#installing-quarto-in-wsl",
    "href": "posts/Why-this-blog/index.html#installing-quarto-in-wsl",
    "title": "Why Blogging? (and How?)",
    "section": "Installing Quarto in WSL",
    "text": "Installing Quarto in WSL\nto install Quarto in WSL (or Ubuntu) via nbdev use:\nmamba install -c fastchan nbdev  \nnbdev_install_quarto"
  },
  {
    "objectID": "posts/understanding-stable-diffusion/index.html",
    "href": "posts/understanding-stable-diffusion/index.html",
    "title": "Understanding Stable Diffusion",
    "section": "",
    "text": "In this blog post I will be presenting a high level explanation of what Stable Diffusion is and how it works. This is the insights I’ve got from Lesson 9 of the fast.ai 2022 part 2 course.\nAccording to Wikipedia Stable Diffusion is a deep learning, text-to-image model released in 2022. It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt. It was developed by researchers from the CompVis Group at Ludwig Maximilian University of Munich and Runway with a compute donation by Stability AI and training data from non-profit organizations."
  },
  {
    "objectID": "posts/understanding-stable-diffusion/index.html#unet",
    "href": "posts/understanding-stable-diffusion/index.html#unet",
    "title": "Understanding Stable Diffusion",
    "section": "1 - UNET:",
    "text": "1 - UNET:\n\n\n\nLet’s examine the input and output of the Unet model and explore methods to accelerate the training process through compression:\n\nInput: The input to the Unet is a somewhat noisy image, ranging from minimally noisy to completely noisy.\nOutput: The model’s output aims to represent the noise pattern present in the input image.\n\nBy subtracting the output noise from the input image, we obtain an approximation of the denoised image.\nCurrently, the handwritten input images are 28x28 in size, but the desired goal is to generate larger images. The Unet models typically work with 512x512x3 images for which millions of noisy versions are used during training. However, training such a model using these large datasets can be time-consuming.\nTo expedite the training process, we can leverage an insightful approach that capitalizes on the fact that pixel values exhibit minimal local variation. Storing each individual pixel value is unnecessary. Instead, we can utilize compression techniques like JPEG, which significantly reduce the amount of storage required to represent an image while retaining essential information. This compression strategy allows us to achieve faster training without compromising the overall image quality."
  },
  {
    "objectID": "posts/understanding-stable-diffusion/index.html#vae---variational-autoencoders",
    "href": "posts/understanding-stable-diffusion/index.html#vae---variational-autoencoders",
    "title": "Understanding Stable Diffusion",
    "section": "2- VAE - Variational autoencoders:",
    "text": "2- VAE - Variational autoencoders:\nNow, let’s explore the process of compressing it using an autoencoder (AE). The autoencoder’s architecture involves progressively halving the number of pixels per dimension and doubling the number of channels at each level using stride two convolutions. Finally, we incorporate a few ResNet-like blocks to further reduce the channel count from 24 to 4.\n\n\n\nWe began with a 512x512x3 image and successfully obtained its compressed representation, known as “latents,” which now has a size of 64x64x4. The compression factor achieved is an impressive 48, resulting in a much smaller representation. This encoding process, which transforms the larger image into a compact form, is carried out by the encoder.\nThe chosen compression factor becomes meaningful based on how effectively we can reconstruct the original image from these 64x64x4 latents. To achieve this, we will develop the inverse process, referred to as the decoder. Once both the encoder and decoder are constructed, they can be combined, and the entire autoencoder can be trained effectively.\nIn summary, the process involves encoding the “big” image into smaller latents (encoder), followed by decoding those latents back to reconstruct the original image (decoder), and finally, training the complete autoencoder with this setup.\n\n\n\nWe can use MSE and train this, in the beginning we will get random outputs but later we should get close to our input\n\n\n\nSo what is the point of a model that spits back an output that is identical to the input?\n\n\n\nThe encoder, depicted in green, is responsible for transforming a larger image into a compact representation. Conversely, the decoder, shown in red, performs the inverse operation, reconstructing the original image from the compressed representation. If I wish to share an image with someone, I can pass it through the encoder, resulting in a representation that is 48 times smaller than the original picture. The recipient, possessing a copy of the trained decoder, can then use it to reverse the process and recover the original image. Essentially, this entire mechanism functions as a compression algorithm, facilitating efficient image transmission and reconstruction.\nTo utilize the compression algorithm effectively, we employ the Unet by passing the compressed “smaller” latents instead of the original “bigger” images as inputs.\nThe updated inputs and outputs of the Unet are as follows: - Input: Latents with some level of noise - Output: Noise\nBy subtracting the output from the input, we obtain denoised latents, which are then fed into the decoder of the autoencoder to generate the best approximation of the denoised image. This autoencoder is called a Variational Autoencoder (VAE).\nIn summary, the process involves starting with a 512x512x3 image, passing it through the VAE’s encoder to obtain compressed latents of size 64x64x4. Subsequently, these latents are passed through the Unet to predict the noise. The noise is then subtracted from the encoder’s latents, resulting in denoised latents. These denoised latents are finally passed through the decoder of the VAE to generate a 512x512x3 image.\nImportant points to consider:\n\nThe VAE serves as an optional building block, allowing faster training of the Unet with smaller-sized latents rather than full images.\nDuring inference, the encoder of the VAE is not required; it is only necessary during the training phase."
  },
  {
    "objectID": "posts/understanding-stable-diffusion/index.html#clip",
    "href": "posts/understanding-stable-diffusion/index.html#clip",
    "title": "Understanding Stable Diffusion",
    "section": "3 - CLIP:",
    "text": "3 - CLIP:\nNow, let’s explore the significance of text prompts in the process. Instead of merely inputting noise and receiving a digit in return, can we experiment with instructing the system to generate a particular number, for instance, “3”?\n\n\n\nTo accomplish this, besides providing the noisy input image, we will also include a one-hot encoded representation of the digit “3”.\n\n\n\nCurrently, we are inputting two elements into this model: the image pixels and the one-hot encoded vector representing the digit it corresponds to. Consequently, the model will learn to predict the noise, benefiting from the extra information about the original digit. This improvement is expected compared to the previous model’s noise prediction capability.\nOnce the model is trained, feeding it the one-hot encoded vector for “3” along with the noise will enable it to recognize the noise that does not represent the number three. This process is referred to as “guidance,” as it helps the model generate the desired image.\nHowever, one might wonder if one-hot encoded vectors are the most efficient approach. For instance, if we wish to create an image from the phrase “a cute teddy,” using one-hot encoded vectors for every phrase could be highly inefficient.\nTo address this, we can develop a model capable of taking the phrase “a cute teddy” as input and generating a vector of numbers, known as embeddings, which somehow represents the characteristics of “cute teddies.”\nIn practice, we can obtain images from the internet, where those with alt tags will have associated text descriptions. These descriptions can be leveraged to create meaningful input for the model’s embeddings, enabling it to understand and generate images that align with the given text description.\n\n\n\nNow we can create two models, one model which is a text encoder and one model which is an image encoder.\n\n\n\nTo achieve our goal, we have two encoders—one for processing images and another for handling text. Each encoder produces two embeddings.\nWhen we pass the image of the swan through the image model, our aim is to obtain embeddings that closely resemble the ones generated by passing the text “the graceful swan” through the text encoder. In essence, we desire similarity between these embeddings. To accomplish this, we leverage the dot product as a measure of similarity. A higher dot product indicates a greater degree of similarity between the embeddings.\n\n\n\nNow, we have a grid containing images and corresponding text, and by computing the dot product of their embeddings, we obtain a score for each combination. Our objective is to achieve high dot product scores (represented by blue diagonal elements) only for matching image-text pairs. Conversely, for non-matching pairs of text and image, we aim to obtain lower dot product scores (depicted by red off-diagonal elements).\n\n\n\nSo our loss function can be defined as adding all the diagonal elements and subtracting from it the off-diagonal elements.\n\n\n\nIf we want this loss function to be good then we’re going to need the weights of our model for the text encoder to spit out embeddings that are very similar to the image embeddings that they’re paired with and not similar to the embedding of the images they are not paired with.\nNow we can feed our text encoder with “a graceful swan”, “some beautiful swan”, “such a lovely swan” and these should all give very similar embeddings because these would all represent very similar images of swans.\nWe’ve successfully created two models that put text and images into the same space, a multimodal(using more than one mode-images and text) model.\nSo we took this detour because creating 1-hot encoded vectors for all the possible phrases was impractical. We can can now take our phrase - “a cute teddy bear” and feed it in text encoder to get out some features/embeddings.\n\n\n\nInstead of using 1-hot encoded vectors as guides during Unet training, we utilize the features generated by the text and image encoders. So, when we input the phrase “a cute teddy” into the text encoder, it produces embeddings that serve as guidance for our model to transform the input noisy image into something resembling “cute teddies” it has encountered before.\nThis pair of models is known as CLIP, which stands for Contrastive Language-Image Pre-training, and the loss function employed is called contrastive loss.\nLet’s now review the building blocks we have established so far.\n\n\n\n\nwe’ve got a Unet that can denoise latents into unnoisy latents\nwe’ve got the decoder of VAE that can take latents and create an image\nwe’ve got the CLIP text encoder which can guide the Unet with captions\n\nStable diffusion is a latent diffusion model and what that means is that it doesn’t operate in the pixel space, it operates on in the latent space of some other autoencoder model and in this case that is a variational autoencoder."
  },
  {
    "objectID": "posts/understanding-stable-diffusion/index.html#additional-things",
    "href": "posts/understanding-stable-diffusion/index.html#additional-things",
    "title": "Understanding Stable Diffusion",
    "section": "4 - Additional Things:",
    "text": "4 - Additional Things:\nThere are two more things Jeremy talks about: - Score function - time-steps\n\n\n\nThese gradients are commonly referred to as the “score function.”\nThe term “time steps” is frequently found in various papers, although we did not employ any actual time steps during our training process. This terminology originated from the initial mathematical formulations. Despite this, we can understand the concept of time steps without it being directly related to real-world time.\nDuring training, we introduced diverse levels of noise to our images, varying from highly noisy to noise-free and even pure noise.\nTo establish a noising schedule, we utilize a monotonically decreasing function. Let’s consider the x-axis (“t”) ranging from one to a thousand. We randomly select a number within this range and use the noise schedule to determine the corresponding sigma (also referred to as beta in some papers). For instance, if we randomly choose the number four, we would look up the corresponding sigma value on the y-axis, representing the amount of noise to add to the image with this randomly chosen value of four.\n\n\n\nDuring the training process, the level of noise added to each image varies depending on a random selection. If a value close to one is chosen, the resulting image will have a substantial amount of noise, whereas a value near one thousand will lead to minimal noise.\nTo achieve this randomness in training, we need to determine a random amount of noise for every image. This can be achieved by selecting a random number between one and a thousand and using a noise scheduler function to obtain the corresponding sigma value for the noise to be added.\nPreviously, people often referred to this random number as the “time step,” but nowadays, the noise scheduler lookup is becoming less prevalent. Many practitioners now simply state the magnitude of noise used.\nDuring the training process, for each mini-batch, a random batch of images is selected from the training set. Additionally, either a random amount of noise or a random “t” value (which is then used to find the amount of noise from the noise scheduler) is chosen. These noisy images are then passed into the model for training, allowing the model’s weights to learn how to predict noise effectively.\n\n\n\nHow do we precisely carry out this inference process? When generating a picture from pure noise, it corresponds to t=0 on the noise scheduler, representing maximum noise. The objective is to teach the model to eliminate noise effectively. However, attempting to accomplish this in a single step could result in poor-quality images.\n\n\n\nIn practice, the noise prediction is multiplied by a constant “c,” akin to a learning rate, but here it is not updating model weights; rather, it updates individual pixels. This updated prediction is then subtracted from the original noisy pixels, resulting in a slightly denoised image.\nHowever, the model does not immediately reach the final denoised image because certain image characteristics, such as those appearing with t=1 (which correspond to poor-quality images), were not encountered in the training set. Consequently, the model lacks knowledge on how to handle such images. To address this, only a small factor of the noise is subtracted, ensuring that the process repeats with somewhat noisy latents, allowing the model to iterate and refine the denoising gradually.\nDeciding the appropriate value for “c” and determining how to perform the subtraction from the noise prediction are vital aspects addressed in the actual diffusion sampler. The sampler manages both the addition of noise and the subsequent subtraction during the diffusion process.\nInterestingly, this approach bears resemblance to deep learning optimizers. For instance, momentum in optimizers suggests increasing the change in parameters when they are changed repeatedly by a similar amount over multiple steps. Similarly, Adam optimizer considers changes in variance. Although diffusion-based models and optimizers stem from different mathematical domains (differential equations versus optimization), they share parallel concepts. Differential equation solvers also focus on taking more significant steps to converge quickly. They often utilize t as an input, a common feature in most diffusion models, although we have yet to discuss it in detail.\n\n\n\nIn diffusion models, it’s a common practice to include not only the input pixels and captions but also the parameter “t.” The intention behind incorporating “t” is to provide the model with information about the amount of noise present, as “t” is associated with the noise level.\nHowever, Jeremy strongly suspects that this premise might be incorrect because for a sophisticated neural network, determining the noise level can be relatively straightforward. Consequently, when the need for passing “t” diminishes, the problem transforms from one resembling differential equations to that resembling optimizers.\nBy swapping Mean Squared Error (MSE) with perceptual loss, various possibilities emerge, expanding the approach to be viewed as an optimization problem rather than merely a differential equation-solving problem. This shift in perspective unlocks new avenues for exploration and potential improvements in the model’s performance."
  },
  {
    "objectID": "posts/breaking_down_backpropagation_code/index.html",
    "href": "posts/breaking_down_backpropagation_code/index.html",
    "title": "Breaking down backpropagation implementation",
    "section": "",
    "text": "I have always had this bad habit while learning where I would just take the informations in passively and not try to apply any of it, even while studying math I only read the theorems and never do exercises.\nThis is a very bad habit that I am trying to get rid of lately while doing the fastai course. I try my best not to let anything slide without explicitly understanding it.\nOr that’s what I thought. Because recently while browsing the fastai forum I stumbled upon a question the code of backpropagation. A concept that I thought I understood very well. The question was and I quote:"
  },
  {
    "objectID": "posts/breaking_down_backpropagation_code/index.html#question",
    "href": "posts/breaking_down_backpropagation_code/index.html#question",
    "title": "Breaking down backpropagation implementation",
    "section": "Question:",
    "text": "Question:\n\nclass Mse():\n    def __call__(self, inp, targ):\n        self.inp,self.targ = inp,targ\n        self.out = mse(inp, targ)\n        return self.out \n\n    def backward(self):\n        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]\n\n\nclass Lin():\n\n    def __init__(self, w, b): self.w,self.b = w,b\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = lin(inp, self.w, self.b)\n        return self.out\n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n\nShouldn’t it be self.out.g instead of self.inp.g in the backward definition of Mse class. I don’t know how Lin backward() automatically gets self.out.g value. Can some one explain?\nEnd of Quote\nWhen I read this question I realized that I didn’t have the answer so I revisited the video for the lesson 13 and came up with a “detailed” response. Here it is:"
  },
  {
    "objectID": "posts/breaking_down_backpropagation_code/index.html#answer",
    "href": "posts/breaking_down_backpropagation_code/index.html#answer",
    "title": "Breaking down backpropagation implementation",
    "section": "Answer:",
    "text": "Answer:\nTo understand this let us first set all the code we need, then we will take it execute it step by step. Our building blocks are: - the lin function: def lin(x, w, b): return x@w + b - the mse function: def mse(output,target): return ((output.squeeze()-target)**2).mean() and the classes: Mse(), ReLU(), Lin() and Model() Now to create our model and compute backpropagation we run the following code:\n\nmodel = Model(w1, b1, w2, b2)\n\nwhat happens now?: We are calling the Model constructor so if we look inside the object model we will find:\n\nmodel.layers = [Lin(w1,b1),Relu(),Lin(w2,b2)]\nmodel.loss = Mse() \n\nLet’s name our layers L1, R, and L2 to make the explanation easier to follow. so L1.w = w1, L1.b = b1, L2.w = w2 and L2.b = b2.\nNow let’s execute the following line:\n\nloss = model(x_train, y_train)\n\nhere we are using the model object as if it was a function, this will trigger the __call__ method, here is the code for it:\n\ndef __call__(self, x, targ):\n        for l in self.layers: x = l(x)\n        return self.loss(x, targ)\n\nlet’s execute it: in our case x = x_train and targ = y_train now let’s go through that for loop: for l in self.layers: x = l(x) the contents of model.layers is [L1,R,L2] so the first instruction will be: x = L1(x) similarly here again we are using L1 as function so let’s go see what’s in its __call__ method and run it:\n\n# Lin Call method\ndef __call__(self, inp):\n        self.inp = inp\n        self.out = lin(inp, self.w, self.b)\n        return self.out\n\nso we are assigning inp to self.inp, in this case L1.inp = x_train and L1.out = lin(inp, w1,b1) = x_train @ w1 + b1.\nThe call method returns self.out so the new value of x will be x = L1.out.\nThe first iteration of the loop is done, next element is the layer R, so x = R(x)\n\n# ReLU call method\ndef __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)\n        return self.out\n\nso now we have R.inp = L1.out R.out = relu(L1.inp) # basically equal to L1.inp when it's > 0, 0 otherwise.\nNow the new value of x is x = relu(L1.inp) The second iteration is done, next element is the layer L2, so x = L2(x) now we have L2.inp = relu(L1.inp) and L2.out = relu(L1.inp) @ w2 + b2.\nThe new value of x is x = L2.out = relu(L1.inp) @ w2 + b2.\nThe for loop has ended. Let’s go to the next line of code:\n\nreturn self.loss(x, targ)\n\nWe saw earlier that model.loss = Mse() so we are using the __call__ method of the Mse class:\n\n# call method of the Mse class\ndef __call__(self, inp, targ):\n        self.inp, self.targ = inp, targ\n        self.out = mse(inp, targ)\n        return self.out\n\nnow we have mse.inp = x, mse.targ = targ and mse.out = mse(x, targ) = ((x.squeeze()-targ)**2).mean().\nThe method return mse.out so loss = mse.out.\nFinally we get to the part which confused us both:\n\nmodel.backward()\n\nit calls the backward method of the Model class:\n\n# backward method of the Model class\ndef backward(self):\n        self.loss.backward()\n        for l in reversed(self.layers): l.backward()\n\nIn the first line we have model.loss.backward() which is none other than the backward method of the Mse class. because remember that loss is an instance of the Mse class.\n\n# backward method of Mse\ndef backward(self):\n        self.inp.g = 2 * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.inp.shape[0]\n\nSo here we compute mse.inp.g and we saw earlier that mse.ing = x so we are in fact computing x.g and it’s equal to x.g = 2 * (x.squeeze() - targ).unsqueeze(-1) / x.shape[0]\nx as you know is the output of our MLP (multi level perceptron), and the gradient of the loss with respect to the output is stored in the output tensor i.e x.g. So that’s why it should be indeed inp.g and not out.g in the backward method of the Mse class.\nNow in order to find out how backward of Lin get the out.g value let’s continue executing our code. We have have executed the first line now let’s run the for loop:\n\nfor l in reversed(self.layers): l.backward()\n\nthe first value of l is L2 (because we are going through the reversed list of layers) so let’s run L2.backward()\n\n# Lin backward\ndef backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        self.w.g = self.inp.t() @ self.out.g\n        self.b.g = self.out.g.sum(0)\n\nWe already know that:\n\nL2.inp = relu(L1.inp)\nL2.out = relu(L1.inp) @ w2 + b2 = x\n\nso when we call L2.backward() this method will perform the following updates:\n\nL2.inp.g =  L2.out.g @ L2.w.t() # which is equivalent to L2.inp.g = x.g @ w2.t() \nw2.g = L2.inp.t() @ L2.out.g\nb2.g = L2.out.g.sum(0)\n\nAs you can see Lin knows automatically what out.g is, because when we ran model.loss.backward() we calculated it. So now we have computed L2.inp.g (which is R.out.g) ,w2.g and b2.g.\nThe first iteration of the loop has ended, next l=R and we will run R.backward:\n\ndef backward(self): self.inp.g = (self.inp>0).float() * self.out.g\n\nWe know that R.inp = L1.out and R.out = relu(L1.inp) The following updates will occur:\n\nR.inp.g = (R.inp > 0).float() * R.out.g \n\nNow we have computed R.inp.g (which is L1.out.g).\nThis iteration is done, next is l = L1 so we will call L1.backward().\nWe know that L1.inp = x_train and that L1.out  = R.inp So calling backward of L1 will give us the following updates:\n\nL1.inp.g =  L1.out.g @ w1.t() # which is equivalent to L1.inp.g = R.inp.g @ w1.t() \nw1.g = L1.inp.t() @ L1.out.g\nb1.g = L1.out.g.sum(0)"
  },
  {
    "objectID": "posts/breaking_down_backpropagation_code/index.html#conclusion",
    "href": "posts/breaking_down_backpropagation_code/index.html#conclusion",
    "title": "Breaking down backpropagation implementation",
    "section": "Conclusion:",
    "text": "Conclusion:\nThe main takeaway is that backpropagation strats at the end and computes the gradient of the loss and stores it in the output tensor of the neural network (which is the input tensor of the loss function, and that’s what’s confusing).\nI really hope that you may find this blog post useful."
  },
  {
    "objectID": "posts/coriander-parsley-classifier/index.html",
    "href": "posts/coriander-parsley-classifier/index.html",
    "title": "A coriander قزبر vs معدنوس Parsley Classifier",
    "section": "",
    "text": "Coriander قزبر and parsley معدنوس are two staple ingredients of Moroccan cuisine, our moms have the superpower of easily distinguishing between them, but us mortals cannot.\nIn lesson 2 of Deep Learning for Coders 2022 part 1, Jeremy presented a simple image classification model for grizzly, black and teddy bears. I will be using a similar model to help us on this strenuous and confusing task."
  },
  {
    "objectID": "posts/coriander-parsley-classifier/index.html#create-a-huggingface-account",
    "href": "posts/coriander-parsley-classifier/index.html#create-a-huggingface-account",
    "title": "A coriander قزبر vs معدنوس Parsley Classifier",
    "section": "2.1 Create a HuggingFace Account",
    "text": "2.1 Create a HuggingFace Account\n\nVisit HuggingFace Website and Create an account\nNow go to HuggingFace Spaces and Create a New Space\nIn the SDK section select Gradio\n\n\nYou have now a new space.\n\nGo to your terminal in install git lfs (to be able to upload large files) using the following command:\ngit lfs install\nNow clone your space to your computer using the command:\ngit clone https://huggingface.co/spaces/<your_username>/<your_spacename>"
  },
  {
    "objectID": "posts/coriander-parsley-classifier/index.html#creating-a-gradio-app",
    "href": "posts/coriander-parsley-classifier/index.html#creating-a-gradio-app",
    "title": "A coriander قزبر vs معدنوس Parsley Classifier",
    "section": "2.2 Creating a Gradio app",
    "text": "2.2 Creating a Gradio app\nNow we should create a Gradio app that takes an input from the user and queries the model for its prediction and return it.\nGradio usually requires a python file named app.py but we will be doing things differently.\nIn the fastai course we used JupyterNotebooks and nbdev to create such file, to do so, create a new Jupyter Notebook (in the folder of your cloned repo) and fill the first cell with this code:\n\n#<add a pipe character here>default_exp app\n\n#|default_exp app\nNow let’s import fastai vision and gradio\n\n#<add a pipe character here>export\nfrom fastai.vision.all import *\nimport gradio as gr\n\nthe first comment in the cell should be #|default_exp app, it is is an indicator for nbdev to export the file and name it “app”. The rest is imports we will need.\nNow move the exported model.pkl model to the folder of your repo and load it.\n\n#<add a pipe character here>export\nlearn = load_learner('model.pkl')\n\nthat #|export line indicates to nbdev that the code of this cell needs to be included in the final exported app.py, if you want to do some tests for example import some images and try the model on them you can do so on cells that do not have the comment on their first line.\nOur model is now up and ready, let’s create a Gradio interface:\n\n#<add a pipe character here>export\ncategories = ('coriander', 'parsley')\ndef classify_image(img):\n    pred, idx, probs = learn.predict(img)\n    return dict(zip(categories,map(float,probs)))\n\nLet’s break this down a bit:\n- categories contains the categories that our classifier chooses from.\nGradio requires us to give him a function that he will call, this function is classify_image, it takes an image as an arguments. the learner has a method predict that takes an input and returns 3 things:\n- pred: True for positive class, False for negative class\n- idx: gives the index of the class\n- probs: gives the probability that this item is in this class for example if we have two categories probs will be a (2,1) array with probs[0] the probability of the item being in class 0, and probs[1] the probability of it being in class 1.\nThe function returns a dictionary with categories as keys and float(probs) as values, if you are not familiar with the dict(zip()) paradigm, check up this video.\nNow that we have our function set, let’s go and create the actual interface:\n\n#<add a pipe character here>export\nimage = gr.inputs.Image(shape=(192,192))\nlabel = gr.outputs.Label()\nexamples = ['coriander.jpg','parsley.jpg']\n\nintf = gr.Interface(fn=classify_image, inputs = image, outputs = label, examples = examples)\nintf.launch(inline=False)\n\nWe created an image input object and a label output object. We also added two images to our main folder of actual coriander and parsley respectively named coriander.jpg and parsley.jpg.\nThen we proceed to creating an interface that takes as parameters:\n- fn: the function classify_image\n- inputs: the input image\n- outputs: the output label\n- examples: this is optional, use it if you want to add examples to help the user.\nOur notebook is ready to be converted into a Gradio app! We will use nbdev to do so add this cell to the bottom of your notebook and run it:\n\nimport nbdev\nnbdev.export.nb_export('app.ipynb', 'app')\nprint('Export successful')\n\nWe are almost done, all we need to do now is to create a requirements.txt file and fill it with the following:\nfastai  \ntorch  \ngradio  \nEverything is good now, let’s push this to HuggingFace:\nYou can either do this from the terminal (reminder: make sure to install git lfs) using the following commands:\ngit add .\ngit commit -m \"add your custom message\"\ngit push\nOr via the HuggingFace Website, Go to your space -> Files and versions. And add your files manually from there.\nWhen everything is done, visit https://huggingface.co/spaces/<your_username>/<your_spacename> and you will have a fully operational coriander and parsley classifying app that you can share with your friends, it’s not perfect but still it’s better than wild guessing.\nHere is mine\nNote: Hugging Face Spaces that run on the default cpu-basic hardware (free tier), will go to sleep if inactive for more than a set time (currently, 72 hours). Anyone visiting your Space will restart it automatically. When it goes to sleep you can restart it by going to your spaces."
  },
  {
    "objectID": "posts/understanding-gradient-descent/index.html",
    "href": "posts/understanding-gradient-descent/index.html",
    "title": "Understanding Gradient Descent",
    "section": "",
    "text": "As you know I am currently doing fast.ai course. And I decided to take my time doing Lesson 3 because it’s about a very important topic: “Gradient Descent”.\nI’ve done many ML courses by the past (Andrew Ng’s Course, Aurelien Geron’s book, etc…) and each of these helped me understand what gradient descent was to a certain extent. But now I can confidently say that I’ve finally grasped what Gradient Descent is truly about.\nYou too can achieve the same understanding by following the steps I did:\n\nWatch the lesson 3 video and follow with Jeremy\nRead Chapter 4 of the fastai book\nWatch Andrej Karpathy’s amazing video: The spelled-out intro to neural networks and backpropagation: building micrograd\n\nI can’t stress enough how amazing Andrej’s video is!!! It’s a perfect complementary material to fast.ai’s lesson 3, and he goes step by step on how to build micrograd which is an autograd engine.\nDon’t forget to write the code along with both Jeremy and Andrej.\nDo this and you will have understood how Gradient Descent works and you will know how to implement it too."
  },
  {
    "objectID": "posts/LLM-Discord-Bot/index.html",
    "href": "posts/LLM-Discord-Bot/index.html",
    "title": "Create a LLM-powered Discord Bot",
    "section": "",
    "text": "In this beginner friendly blog post we are going to build an LLM-powered Discord Bot using llama.cpp, here are the steps we are going to follow: 1. Download a LLM from huggingface 2. Setup a REST API to use this model 3. Create a Discord Bot Application 4. Use the REST API to operate the bot\nThis is a beginner friendly tutorial and assumes only basic knowledge of Python and Linux. I will be working in Ubuntu 20.04 installed on WSL2 but any other Linux distribution should work too."
  },
  {
    "objectID": "posts/LLM-Discord-Bot/index.html#tools",
    "href": "posts/LLM-Discord-Bot/index.html#tools",
    "title": "Create a LLM-powered Discord Bot",
    "section": "1.1 Tools:",
    "text": "1.1 Tools:\nLet’s start first with an update to apt to get the latest metadata for Ubuntu packages. Open your Linux/WSL2 terminal and type the following command:\n\nsudo apt-get update\n\nNow let’s install some packages that we’re going to need.\n\nsudo apt-get install python3-virtualenv python3-pip curl jq \n\n\nvirtualenv is a tool to create isolated Python environments. It creates a folder which contains all the necessary executables to use the packages that a Python project would need. It’s better to create a virtual environment for each project so that even if we mess up our installations, the mess wouldn’t spread to all of the system.\n\npip is the standard package manager for Python is pip . It allows you to install and manage packages that aren’t part of the Python standard library.\ncurl curl (short for “Client URL”) is a command line tool that enables data transfer over various network protocols. It communicates with a web or application server by specifying a relevant URL and the data that need to be sent or received.\njq is a lightweight and flexible command-line JSON processor that we will use to parse and format replies from our API.\nNow that we installed pip let’s install more tools that we’re going to need:\n\n\npip install discord.py python-dotenv\n\n\ndiscord.py is a Python library that exhaustively implements Discord’s APIs in an efficient and Pythonic way. This includes utilizing Python’s implementation of Async IO.\npython-dotenv eads key-value pairs from a . env file and can set them as environment variables. It helps in the development of applications following the 12-factor principles."
  },
  {
    "objectID": "posts/LLM-Discord-Bot/index.html#the-model",
    "href": "posts/LLM-Discord-Bot/index.html#the-model",
    "title": "Create a LLM-powered Discord Bot",
    "section": "1.2 The Model:",
    "text": "1.2 The Model:\nWe are going to use llama.cpp so you should download a model that is compatible with it. You can browse HuggingFace and check the model card to see if it is compatible with llama.cpp. Another way is to download models with the GGUF extension.\nI am going to be using the Llama-2-7B-Chat-GGUF by TheBloke available here.\nIn the model card we can see that there are many versions of that model, the difference between each version is the quantisation method, you can see the size of each version and how much RAM it needs to operate. I downloaded the llama-2-7b-chat.Q5_K_M.gguf version.\nTo download a model you must go to the Files and Versions Tab and right-click on the model you want then choose Copy Link.\nNow type you can download using the wget command in the terminal like this:\n\nwget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf\n\nIf you don’t have wget already installed, you can install it by simply typing:\n\nsudo apt-get install wget\n\nIf you want a better quality of replies and also have the adequate hardware for it, you can download larger models (i.e. 13B and higher)."
  },
  {
    "objectID": "posts/LLM-Discord-Bot/index.html#setting-up-the-project-files",
    "href": "posts/LLM-Discord-Bot/index.html#setting-up-the-project-files",
    "title": "Create a LLM-powered Discord Bot",
    "section": "1.3 Setting up the project files:",
    "text": "1.3 Setting up the project files:\nNow let’s create a directory for our project.\nWe can do that by the following command:\n\nmkdir discord-llm-bot\n\nNow let’s access our newly created folder using:\n\ncd discord-llm-bot\n\nWe create a virtual environment that we will call env:\n\nvirtualenv env\n\nOnce created, we need to activate our environment\n\nsource env/bin/activate\n\nYou will see now that your terminal has a (env) it means that your virtual environment is active.\nWe will be using Python bindings for LLAMA-CPP. In the Python bindings page, go down to the Web Server section and use the following command to install the Python Bindings:\n\npip install \"llama-cpp-python[server]\""
  },
  {
    "objectID": "posts/LLM-Discord-Bot/index.html#chat.py",
    "href": "posts/LLM-Discord-Bot/index.html#chat.py",
    "title": "Create a LLM-powered Discord Bot",
    "section": "2.1 chat.py",
    "text": "2.1 chat.py\nWriting curl requests each time is tedious, let’s write a script to do make life easier for us:\nFor our script we will use the aiohttp. The quickstart example is enough for what we want to do.\nLet’s copy the imports and the first example into our new script file that we will create in our project directory and we will call it chat.py:\n\nnvim chat.py\n\nNow when we paste the imports and first example our file is like that:\n\nimport aiohttp\nimport asyncio\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        async with session.get('http://httpbin.org/get') as resp:\n            print(resp.status)\n            print(await resp.text())\n\nasyncio.run(main())\n\nWe must modify few things to make this work: First we create the following variables: must use a POST request instead of a GET request in line 5.\n\nAPI_URL = 'http://localhost:8000/v1/chat/completions'\npayload = {\n    \"max_tokens\":512,\n    \"messages\": [\n        {\n            \"content\": \"Tell me what quantum physics is about? ### Response: \",\n            \"role\": \"user\"\n        }\n    ]\n}\nheaders = {\"Content-Type\":\"application/json\"}\n\nThen we need to use a POST request instead of a GET request and we also need to specify our API_URL, headers, and payload like this:\n\nasync with session.get(API_URL, data = payload, headers = headers) as resp:\n\nNow in order for our payload to get parsed correctly we need to dump it as a string for the data argument so we import json and use data = json.dumps(payload) method to do so.\nOur chat.py should look like this:\n\nimport json\nimport aiohttp\nimport asyncio\nAPI_URL = 'http://localhost:8000/v1/chat/completions'\npayload = {\n    \"max_tokens\":512,\n    \"messages\": [\n        {\n            \"content\": \"What is the Capital of Japan? ### Response: \",\n            \"role\": \"user\"\n        }\n    ]\n}\nheaders = {\"Content-Type\":\"application/json\"}\n\nasync def main():\n    async with aiohttp.ClientSession() as session:\n        async with session.post(API_URL, data = json.dumps(payload), headers = headers) as resp:\n            print(resp.status)\n            print(await resp.text())\n\nasyncio.run(main())\n\n(I changed the prompt because the one about Quantum Physics takes so much time to respond.) let’s save it and run it:\n\npython3 chat.py\n\nthis gives us the following result:\n{\"id\":\"chatcmpl-f619a4ee-8509-4029-8b31-20b65db84ffd\",\"object\":\"chat.completion\",\"created\":1699293282,\"model\":\"/home/llama-2-7b-chat.Q5_K_M.gguf\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\" The capital of Japan is Tokyo.\"},\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":35,\"completion_tokens\":7,\"total_tokens\":42}}\nLet’s modify our script to print just the desired reply and also so that we don’t have to edit our script each time we want to use a different prompt.\nLet’s first see what the script would look like and then explain our changes.\n\nimport json\nimport aiohttp\nimport asyncio\n\nAPI_URL = 'http://localhost:8000/v1/chat/completions'\nheaders = {\"Content-Type\":\"application/json\"}\n\nasync def main():\n    prompt = input(\"User: \")\n    payload = {\n        \"max_tokens\":512,\n        \"messages\": [\n            {\n                \"content\": f\"{prompt} ### response: \",\n                \"role\": \"user\"\n                }\n            ]\n        }\n    async with aiohttp.ClientSession() as session:\n        async with session.post(API_URL, data = json.dumps(payload), headers = headers) as resp:\n            reply = await resp.json()\n            reply_content = reply[\"choices\"][0][\"message\"][\"content\"]\n            print(reply_content)\n\nasyncio.run(main())\n\n\nFirst we moved the payload variable into the main() function.\nThen we created a new variable prompt that takes the input of the User and uses it as a prompt, you can see that this variable is used in the content field of the payload (line 14).\nWe modified the print statement in the session (line 21), it is now a variable called reply, this variable contains the whole reply.\nWe proceed to take only what matters to us (line 22) which is the text in the content field, and we put it in the reply_content variable.\nNow let’s save chat.py and run it again with python3 chat.py\nI gave it the prompt Give me 3 ancient African cities and the output was:\n\n Sure! Here are three ancient African cities that were important centers of trade, culture, and civilization:\n\n1. Memphis, Egypt - Founded around 2925 BCE by the pharaoh Narmer, Memphis was the capital of ancient Egypt and one of the most important cities in the ancient world. It was located on the west bank of the Nile River and was known for its impressive architecture, including the Great Sphinx and the Pyramids of Giza.\n2. Axum, Ethiopia - Axum was a major center of trade and commerce in ancient Africa, located in what is now modern-day Ethiopia. Founded around 100 CE, it was known for its advanced agriculture, architecture, and engineering. The city was also an important hub for the spread of Christianity throughout East Africa.\n3. Timbuktu, Mali - Located in what is now modern-day Mali, Timbuktu was a major center of trade and learning in West Africa during the medieval period. Founded around 1100 CE, it was known for its universities and libraries, which were renowned throughout the Islamic world. The city was also an important center for the production of books and manuscripts.\nThese three cities were all major centers of trade, culture, and civilization in their respective regions during ancient times, and they played important roles in shaping the history of Africa and the wider world."
  },
  {
    "objectID": "posts/LLM-Discord-Bot/index.html#having-a-conversation",
    "href": "posts/LLM-Discord-Bot/index.html#having-a-conversation",
    "title": "Create a LLM-powered Discord Bot",
    "section": "2.2 Having a conversation:",
    "text": "2.2 Having a conversation:\nWhen you run chat.py you will remark that the conversation ends after the first response, this is not ideal for a chatbot. Let’s modify chat.py to have longer conversations.\n\nimport json\nimport aiohttp\nimport asyncio\n\nAPI_URL = 'http://localhost:8000/v1/chat/completions'\nheaders = {\"Content-Type\":\"application/json\"}\n\nasync def main():\n    while True:\n        prompt = input(\"User: \")\n        payload = {\n        \"max_tokens\":512,\n        \"messages\": [\n            {\n                \"content\": f\"{prompt} ### response: \",\n                \"role\": \"user\"\n            }\n                    ]\n        }\n        async with aiohttp.ClientSession() as session:\n            async with session.post(API_URL, data = json.dumps(payload), headers = headers) as resp:\n                reply = await resp.json()\n                reply_content = reply[\"choices\"][0][\"message\"][\"content\"]\n                print(f\"Bot: {reply_content}\")\n\nasyncio.run(main())\n\nWe wrapped our code in a while statement so that we can maintain a conversation for as long as we desire, we also modified the print statement of the reply to show that it’s the Bot talking.\nNow run chat.py again and you can chat as long as you want. You can use CTRL + C to terminate the program. Here is an example of chat session I had with the bot:\nUser: What is the largest city in Africa?\nBot:  The largest city in Africa is Lagos, Nigeria. With a population of over 21 million people, Lagos is not only the largest city in Africa but also one of the fastest-growing cities in the world. It is located in the southwestern part of Nigeria and is known for its vibrant culture, diverse economy, and rich history.\nUser: What is the second largest one?\nBot:  The second largest planet in our solar system is Jupiter.\nAs you can see we can now chat with our Bot but unfortunately he can’t keep track of the context of the conversation. Let’s fix that."
  },
  {
    "objectID": "posts/LLM-Discord-Bot/index.html#handling-context",
    "href": "posts/LLM-Discord-Bot/index.html#handling-context",
    "title": "Create a LLM-powered Discord Bot",
    "section": "2.3 Handling context:",
    "text": "2.3 Handling context:\nOur model is not following the context of the conversation, this is because the OpenAPI implementation (which we are using here) requires the whole conversation to be passed as a prompt in each request. You can read more about this here. Now let’s modify our chat.py script:\n\nimport json\nimport aiohttp\nimport asyncio\n\nAPI_URL = 'http://localhost:8000/v1/chat/completions'\nheaders = {\"Content-Type\":\"application/json\"}\n\npayload = {\n\"max_tokens\":512,\n\"messages\": []\n}\nasync def main():\n    while True:\n        prompt = input(\"User: \")\n        msg = {\n            \"content\": f\"{prompt} ### response: \",\n            \"role\": \"user\"\n        }\n        payload[\"messages\"].append(msg)\n        async with aiohttp.ClientSession() as session:\n            async with session.post(API_URL, data = json.dumps(payload), headers = headers) as resp:\n                reply = await resp.json()\n                reply_content = reply[\"choices\"][0][\"message\"][\"content\"]\n                print(f\"Bot: {reply_content}\")\n        msg_idx = payload[\"messages\"].index(message)\n        payload[\"messages\"][msg_idx][\"content\"] += reply_content\n\n\nasyncio.run(main())\n\nFirst we took out payload out of the while loop (line 8 to 11), it has now an empty list in the messages field that we are going to fill with our conversation as it progresses.\nWe then created a msg variable (line 15) which is going to contain the prompt the user just typed. This msg will be then appended to the payload’s messages field (line 19). The rest of the code is similar and will proceed to give a response i.e reply_content. Now we need to take this response and append its corresponding prompt in the payload’s messages field. We can do that by first getting the index of our actual message (line 25), then we will add that response to the prompt (line 26).\nNow save and run the script again. We can now see that our model can keep up with us:\nUser: What is the largest city in Africa?\nBot:  The largest city in Africa is Lagos, Nigeria. It has a population of over 20 million people, according to estimates in 2020.\nUser: What is the second largest one?\nBot:  The second largest city in Africa is Cairo, Egypt. It has a population of approximately 20 million people, according to estimates in 2020.\nNow we are good to go. Let’s put everything together and create a Discord Bot."
  },
  {
    "objectID": "posts/LLM-Discord-Bot/index.html#creating-a-discord-app",
    "href": "posts/LLM-Discord-Bot/index.html#creating-a-discord-app",
    "title": "Create a LLM-powered Discord Bot",
    "section": "3.1 Creating a Discord App:",
    "text": "3.1 Creating a Discord App:\n\nGo to this link https://discord.com/developers/applications\nLog in or create a new account if you don’t have any.\nClick on New Application (Top Right)\nProvide a Name to your app.\nAccept the terms\nClick Create\n\nNow in the General Information give a name to your bot and a description if you desire, you can also upload a profile picture for it.\n\nClick Save Changes\n\nNow on the left go to Bot section\n\nClick on Reset Token button (confirm the message)\nClick Copy to copy it\n\nNow go back to your project directory and create a new file called .env and paste your token in it with TOKEN= in the beginning.\nYou .env file should look like this:\n\nTOKEN=<HERE IS WHERE YOU WILL PAST YOUR TOKEN>\n\nSave and close this file.\nNow back on the Bot configuration page you need to changes two things: - Toggle off the Public Bot button. - Toggle on the MESSAGE CONTENT INTENT button. As shown here \nNow go to the OAuth2 tab, we need to generate a URL that will help us invite our bot to our server. - Go to URL Generator and tick the bot box. As shown in the following screenshot  - Now tick the following permissions: > - Send Messages > - Create Public Threads > - Create Private Threads > - Send Messages in Threads > - Send TTS Messages > - Read Message History > - Use External Emojis > - Use External Stickers > - Add Reactions\nAs shown in the following screenshot \nNow go to the bottom and copy the generated URL and open it in a new browser tab\nIf this is your first time using Discrod, you must create a server to add your Bot to, you can do that by following this tutorial.\nOnce you have a server in your account\n- Open the generated URL copied previously in a new tab.\n- Select your server.\n\n- Click Continue - Authorize the permissions again - Prove your Humanity! (easier said than done :( )\nNow you are in the server with your bot!\n\nOur Discord Bot is created, now all we have left to do is to ‘connect’ it with our model."
  },
  {
    "objectID": "posts/LLM-Discord-Bot/index.html#llm-discord-bot",
    "href": "posts/LLM-Discord-Bot/index.html#llm-discord-bot",
    "title": "Create a LLM-powered Discord Bot",
    "section": "3.2 LLM Discord Bot:",
    "text": "3.2 LLM Discord Bot:\nNow we need to write a script that would connect our Discord Bot to our local LLM model.\nLet’s go back to the terminal and create a copy of the chat.py and name it bot.py because we will need most of the code we wrote previously.\n\ncp chat.py bot.py\n\n\nOpen bot.py.\n\nFor the code of our bot we will use an example from discord.py repository which can be found in this link: https://github.com/Rapptz/discord.py/blob/master/examples/reply.py\n\nCopy all the code in reply.py and paste it at the end of your bot.py file\n\nNow move import discord to the top\nRemove the original asyncio.run(main())\nRemove the comment from the example.\nMove all the code in the while loop under await message.reply('Hello!', mention_author=True) and indent it properly\nMove await message.reply('Hello!', mention_author=True) under print(f\"Bot: {reply_content}\") and indent it properly\nDelete print(f\"Bot: {reply_content}\")\nEdit await message.reply('Hello!', mention_author=True) to await message.reply(reply_content, mention_author=True)\nDelete the async def main function\nIn if message.content.startswith('!hello'): replace !hello with !bot\nReplaceprompt = input(\"User: \") with stripped_msg = str(message.content).replace('!bot','').strip()\nNow in the msg variable, change {prompt} to {stripped_mgs}\n\nWe are almost done, we need to add our token that we put in the .env file earlier, to do so we need to add the following code to the top of the file:\n\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nTOKEN = os.getenv(\"TOKEN\")\n\nand at the end of the file edit client.run('token') to client.run(TOKEN)\nYour bot.py file shoud look like this:\n\nimport json\nimport aiohttp\nimport asyncio\nimport discord\nimport os\n\nfrom dotenv import load_dotenv \nload_dotenv()\nTOKEN = os.getenv(\"TOKEN\")\n\nAPI_URL = 'http://localhost:8000/v1/chat/completions'\nheaders = {\"Content-Type\":\"application/json\"}\n\npayload = {\n    \"max_tokens\":512,\n    \"messages\": []\n}\n\nclass MyClient(discord.Client):\n    async def on_ready(self):\n        print(f'Logged in as {self.user} (ID: {self.user.id})')\n        print('------')\n\n    async def on_message(self, message):\n        # we do not want the bot to reply to itself\n        if message.author.id == self.user.id:\n            return\n\n        if message.content.startswith('!bot'):\n            stripped_msg = str(message.content).replace('!bot','').strip()\n            msg = {\n                \"content\": f\"{stripped_msg} ### response: \",\n                \"role\": \"user\"\n            }\n            payload[\"messages\"].append(msg)\n            async with aiohttp.ClientSession() as session:\n                async with session.post(API_URL, data = json.dumps(payload), headers = headers) as resp:\n                    reply = await resp.json()\n                    reply_content = reply[\"choices\"][0][\"message\"][\"content\"]\n                    await message.reply(reply_content, mention_author=True)\n            msg_idx = payload[\"messages\"].index(msg)\n            payload[\"messages\"][msg_idx][\"content\"] += reply_content \n\n\nintents = discord.Intents.default()\nintents.message_content = True\n\nclient = MyClient(intents=intents)\nclient.run(TOKEN)\n\nSave it and run it, you should a message like this in the terminal:\n2023-11-07 17:10:33 INFO     discord.client logging in using static token\n2023-11-07 17:10:34 INFO     discord.gateway Shard ID None has connected to Gateway (Session ID: ff89fsdf2342502081fe39).\nLogged in as llm-bot#8912 (ID: 1173453534535)\n------\n\npython bot.py\n\nGo back to your Discord Client, you will see that the bot is now Online.\nTo interact with the bot you must start your message with !bot that’s the flag that will prompt the bot to answer.\nCongratulations you have now a working LLM Discord Bot!\n\nYou can find the code used in this tutorial in my GitHub Repository\nIf you have any questions feel free to Contact Me"
  },
  {
    "objectID": "posts/fastai-deep-learning-for-coders-part-1/index.html",
    "href": "posts/fastai-deep-learning-for-coders-part-1/index.html",
    "title": "Summing up fastai’s Deep Learning for Coders Part 1",
    "section": "",
    "text": "In the ever-evolving world of artificial intelligence, deep learning has emerged as a powerful tool for solving complex problems. However, getting started with deep learning can be a daunting task, especially for coders without prior experience in this field. Fast.ai’s Practical Deep Learning for Coders course, offers an accessible and hands-on approach to learning deep learning concepts. In this blog post, we’ll explore the first part of the course, highlighting its key elements and sharing the invaluable insights gained from this immersive learning experience.\n\nA Practical Approach to Learning Deep Learning:\nFast.ai’s approach to teaching deep learning is unique. Instead of diving straight into the theory and mathematics behind neural networks, the course focuses on hands-on coding exercises and real-world applications. This approach enables coders to build and deploy their own deep learning models from day one, fostering a deeper understanding of the underlying concepts.\n\n\nFastai Library: Simplifying Deep Learning:\nAt the core of the course is the fastai library, a high-level deep learning framework built on top of PyTorch. This library abstracts away much of the complexity associated with training neural networks, making it easier for coders to experiment with different architectures and techniques. With its comprehensive set of pre-built functions, the fastai library allows learners to rapidly prototype and iterate their models.\n\n\nImage Classification and Data Augmentation:\nThe first part of the course delves into image classification, a fundamental problem in computer vision. Through a series of engaging lessons and coding exercises, learners are introduced to concepts such as convolutional neural networks (CNNs) and transfer learning. Moreover, the course emphasizes the importance of data augmentation techniques, enabling learners to improve model performance by generating synthetic training data. Jeremy also gives a plethora of advises regarding both the training and data augmentation phases.\n\n\nCollaborative Filtering and Natural Language Processing:\nBeyond computer vision, the course also covers collaborative filtering and natural language processing (NLP). With collaborative filtering, learners gain insights into recommender systems, enabling them to build personalized recommendation engines. In the NLP section, participants learn how to process and analyze text data using techniques like tokenization, language modeling, and sentiment analysis. These topics broaden the scope of the course, demonstrating the versatility of deep learning in various domains.\n\n\nLive Coding Session:\nDo not forget the “hidden” and most important part in my opinion of this amazing course: The live coding sessions. Jeremy walks you through how he went through a kaggle competition to get to the first place while explaining how to setup a proper working environment for kaggle competition and deep learning in general. YOU SHOULD NOT MISS THIS!!!\n\n\nConclusion:\nFast.ai’s Practical Deep Learning for Coders - Part 1 offers a remarkable entry point into the world of deep learning for coders. Through its hands-on approach, practical examples, and emphasis on real-world applications, the course equips learners with the necessary skills to build and deploy deep learning models. By leveraging the power of the fastai library and gaining a deep understanding of fundamental concepts, participants develop the confidence to explore and experiment with various domains within deep learning. If you’re a coder interested in venturing into the exciting realm of deep learning, Fast.ai’s course is undoubtedly an excellent starting point on your journey."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hamza's Blog",
    "section": "",
    "text": "Create a LLM-powered Discord Bot\n\n\n\n\n\n\n\nllm\n\n\nml\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2023\n\n\nHamza\n\n\n\n\n\n\n  \n\n\n\n\nBreaking down backpropagation implementation\n\n\n\n\n\n\n\nfastai\n\n\nml\n\n\nbackpropagation\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2023\n\n\nHamza ES-SAMAALI\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Stable Diffusion\n\n\n\n\n\n\n\nfastai\n\n\nml\n\n\nstable diffusion\n\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2023\n\n\nHamza ES-SAMAALI\n\n\n\n\n\n\n  \n\n\n\n\nSumming up fastai’s Deep Learning for Coders Part 1\n\n\n\n\n\n\n\nfastai\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2023\n\n\nHamza ES-SAMAALI\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding Gradient Descent\n\n\n\n\n\n\n\ngradient\n\n\ndescent\n\n\nneural\n\n\nnetwork\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2023\n\n\nHamza ES-SAMAALI\n\n\n\n\n\n\n  \n\n\n\n\nA coriander قزبر vs معدنوس Parsley Classifier\n\n\n\n\n\n\n\nfastai\n\n\nml\n\n\nclassification\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\nHamza\n\n\n\n\n\n\n  \n\n\n\n\nWhy Blogging? (and How?)\n\n\n\n\n\n\n\nblogging\n\n\nquarto\n\n\njupyter\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\nHamza Es-samaali\n\n\n\n\n\n\nNo matching items"
  }
]